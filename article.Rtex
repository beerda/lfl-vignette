\documentclass[article]{jss}

\usepackage{thumbpdf,lmodern}
\usepackage{amsmath}

\newcommand{\lfl}{\pkg{lfl}}
\newcommand{\R}{\proglang{R}}
\newcommand{\todo}[1]{{\color{red} TODO: #1}}

\newcommand{\tgodel}{{T}_{\textrm{M}}}
\newcommand{\tluk}{{T}_{\textrm{\L}}}
\newcommand{\tgoguen}{{T}_{\textrm{P}}}
\newcommand{\sgodel}{{C}_{\textrm{M}}}
\newcommand{\sgoguen}{{C}_{\textrm{P}}}
\newcommand{\sluk}{{C}_{\textrm{\L}}}
\newcommand{\rgodel}{{R}_{\textrm{M}}}
\newcommand{\rgoguen}{{R}_{\textrm{P}}}
\newcommand{\rluk}{{R}_{\textrm{\L}}}
\newcommand{\bgodel}{{B}_{\textrm{M}}}
\newcommand{\bgoguen}{{B}_{\textrm{P}}}
\newcommand{\bluk}{{B}_{\textrm{\L}}}
\newcommand{\ngodel}{{N}_{\textrm{M}}}
\newcommand{\ngoguen}{{N}_{\textrm{P}}}
\newcommand{\nluk}{{N}_{\textrm{\L}}}




%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Achim Zeileis\\Universit\"at Innsbruck
   \And Second Author\\Plus Affiliation}
\Plainauthor{Achim Zeileis, Second Author}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\lfl{}: Linguistic Fuzzy Logic in \R{}}
\Plaintitle{lfl: Linguistic Fuzzy logic in R}
\Shorttitle{\lfl{}: Linguistic Fuzzy Logic in \R{}}

%% - \Abstract{} almost as usual
\Abstract{
  This paper presents an \R{} package that enables the use of linguistic
  fuzzy logic in data processing applications. The \lfl{} package provides tools for
  transformation of data into fuzzy sets representing linguistic expressions, mining
  of linguistic fuzzy association rules, perfoming an inference on fuzzy rule bases
  using the Perception-based logical deduction (PbLD), and computing compositions of
  fuzzy relations. The package also enables to use the Fuzzy rule-based ensemble,
  a tool for time series forecasting based on an ensemble of forecasts from several individual
  methods implemented in \R{}.
}


\Keywords{fuzzy sets, fuzzy natural logic, linguistic fuzzy logic, association rules, compositions of relations, \R{}}
\Plainkeywords{fuzzy sets, fuzzy natural logic, linguistic fuzzy logic, association rules, compositions of relations, R}


%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Achim Zeileis\\
  Journal of Statistical Software\\
  \emph{and}\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section{Introduction} \label{sec:intro}


The aim of this paper is to present a new package for the \R{} statistical environment
\citep{R2020} that enables the use of the linguistic fuzzy logic in data processing
applications. The package provides results of original work of
\cite{Novak08, Novak:PbLD, DvoStep:PbLD2015, StepBurda:FRBE_FSS}, and others, and it provides
executable routines that are not freely available elsewhere.

Indeed, there already exist several packages for \R{} that are focused on vagueness and fuzziness. For
instance, the \pkg{sets}
package \citep{setsPkg} introduces many basic operations on fuzzy sets, the \pkg{FuzzyNumbers}
package \citep{FuzzyNumbersPkg} provides classes and methods to deal with fuzzy numbers,
the \pkg{SAFD} package \citep{safdPkg} contains tools for elementary statistics on fuzzy data, and
the \pkg{fclust} \citep{fclustPkg} brings the fuzzy K-Means clustering technique to the environment of
the \R{} system.

The \lfl{} package described in this paper focuses on creation of systems based on fuzzy logic and their
usage in classification and prediction. A similar task is performed also by the \pkg{fugeR}
package \citep{fugeRPkg} that introduces an evolutionary algorithm for a construction of a fuzzy system from
a training data set, or by the \pkg{frbs} package \citep{frbsPkg} that provides many widely
accepted approaches for building the fuzzy systems, based on space partition, neural networks,
clustering, gradient descent, or genetic algorithms.

The algorithms provided by the \lfl{} package are tightly connected with the notion of
the \emph{fuzzy natural logic} (FNL), formerly also called the \emph{linguistic fuzzy logic} (LFL),
that was initially developed by \cite{Novak08}.
A central notion of the fuzzy natural logic is the expression of the form
%
$$\langle \textrm{linguistic hedge}\rangle \langle \textrm{atomic expression}\rangle$$
%
such as ``very small'', ``roughly medium'', or ``extremely big'', where atomic expression (such as
``small'' or ``big'') denotes some vague quantity that could be extended with a linguistic hedge
(such as ``very'' or ``rather'') that further adjusts the vagueness of the whole expression.
\emph{Fuzzy sets} provide a mathematical framework for manipulation and reasoning with
such linguistic expressions. On top of that, a specific inference method, \emph{Perception-based
Logical Deduction} (PbLD), was developed by \cite{Novak08}.

As opposed to the traditional approach of \cite{MamdaniAssilian75}, who build the
rule base as a disjunction of conjunctions of antecedents and consequents, the PbLD approach is
closer to the implicative approach since it handles
the rules as implications and rule bases as conjunctions of these implications.
Thus, PbLD allows a rule base of implicative linguistic IF-THEN
rules to be inferred from while taking care of the \emph{specificity} of the rule.
For instance, a rule with antecedent an ``age is very small'' is more specific than a rule
with an antecedent ``age is small'' since everything that is very small is also small, but not vice versa.
In PbLD, more specific
rules take precedence over more general rules, if both of them fire in degree 1. That enables e.g.
non-continuous changes to be implemented on the output of the inference, as many times is needed by
the application.
We refer to \cite{Novak:PbLD,DvoStep:PbLD2015} for all the details on PbLD.

Other software, although not connected with the \R{} statistical environment but dealing with the
similar topic as the \lfl{} package, is \emph{Linguistic Fuzzy Logic Controller} (LFLC) by \cite{dvo:lflc}.
However, unlike LFLC, the \lfl{} package is free and open-source.

The \lfl{} package also provides functions for searching for fuzzy association rules. Together
with PbLD, they can be used as a machine learning tool for classification or regression problems.
The package also includes the \emph{Fuzzy Rule-based Ensemble}, a tool for time series forecasting
\citep{frbe2014}, which is built on top of the fuzzy association rules search algorithm and PbLD.

Alternatively to machine learning, classification tasks may be solved based on human expert knowledge
by using the technique of \emph{compositions of fuzzy relations}, which were thoroughly studied e.g. by
\cite{Step2014,Cao2017b,Cao2017}.




\subsection{Overview of the Paper}

The rest of the paper is organized as follows. \todo{doplnit}




\subsection[How to Obtain the lfl Package]{How to Obtain the \lfl{} Package} \label{sec:obtaining}

To obtain the \lfl{} package, a working instance of the \R{} statistical environment has to be installed
first and then the
%
\input{chunks/chunk-cran.lfl}
%
command automatically downloads the latest stable version of the \lfl{} package from CRAN together with all
its dependencies, compiles, and installs it. The \lfl{} package works on all platforms supported by
the \R{} software including Microsoft Windows, GNU/Linux, and MacOS. Alternatively, the development
version may be installed directly from GitHub by issuing following commands within the R session:
%
\input{chunks/chunk-git.lfl}
%
After the installation is successfull, the following command
causes loading of the package into the working space so that the user can start using it:
%
\input{chunks/chunk-load.lfl}
%



%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)



\section{Fuzzy Logic and Fuzzy Sets} \label{sec:fuzzysets}

\emph{Triangular norm} (t-norm) is a function $\otimes: [0,1] \times [0,1] \to [0,1]$,
which is associative, commutative, monotone increasing (in both
places), and which satisfies the boundary conditions $\alpha \otimes 0 = 0$ and
$\alpha \otimes 1 = \alpha$ for each $\alpha \in [0, 1]$.
In fuzzy logic, a t-norm $T$ is used to model the (strong) \emph{conjunction}.

For any left-continuous t-norm $\otimes$, there is a unique binary operation $\Rightarrow$
called the \emph{residuum} of the t-norm $\otimes$ such that
%
$$\gamma \otimes \alpha \le \beta\quad\textrm{if and only if}\quad \gamma \le \alpha \Rightarrow \beta.$$
%
Residuum $\Rightarrow$ is used to model the \emph{implication} in fuzzy logic. It can be also
interpreted as a fuzzy version of the \emph{modus ponens} inference rule.

Truth functions of other logical connectives may be defined based on t-norm $\otimes$
and its residuum $\Rightarrow$. For instance, the \emph{residual negation} $\lnot$
and \emph{bi-residual equivalence} (\emph{bi-residuum}) $\Leftrightarrow$ are
defined as follows:
%
\begin{align*}
\lnot\alpha &= \alpha \Rightarrow 0; \\
\alpha \Leftrightarrow \beta &= (\alpha \Rightarrow \beta) \otimes (\beta \Rightarrow \alpha).
\end{align*}

For some t-norms, the residual negation $\lnot$ is not \emph{involutive}, that is,
it does not obey the law of double negation $\lnot\lnot\alpha = \alpha$. In such
cases the involutive negation is sometimes added to the fuzzy logic as an additional
connective $\sim$.

A t-norm $\otimes$ together with the involutive negation $\sim$ is used to define a \emph{triangular conorm}
(t-conorm), which model a (strong) \emph{disjunction} $\oplus$ in fuzzy logic:
%
$$\alpha\oplus\beta = {\sim}({\sim}\alpha \otimes {\sim}\beta).$$

Usually, two more operators are often needed, which are connected to the
domain of truth values: the lattice operations \emph{infimum} (meet) $\land$ and
the \emph{supremum} (join) $\lor$. They are usually used to interpret the so-called
connectives of weak conjunction and the weak disjunction, respectively.

T-norms and t-conorms play a crucial role also in the theory of fuzzy sets. Fuzzy sets
extend ordinary sets by allowing a partial membership of an element to the set.
A \emph{fuzzy set} $X$ from a universe $U$ is defined by a \emph{membership function} $X: U \to [0, 1]$
that specifies for each element $u\in U$ a degree $X(u)$ of membership of the element $u$ in the fuzzy set $X$.
The degree of membership is a number from the interval $[0, 1]$ where $0$ means $u \notin X$ and $1$
means full membership $u \in X$. We do not distinguish between a fuzzy set and its
membership function, that is, $X(u)$ denotes the degree of membership of the element $u$ in the
fuzzy set $X$. (Clearly ordinary sets are special cases of fuzzy sets such that their membership
function maps $U$ to $\{0, 1\}$.) A \emph{cardinality} $|X|$ of a fuzzy set $X$ is defined as
the sum of the membership degrees \citep{novak1999}:
%
$$|X| = \sum_{\forall u\in U} X(u).$$

T-norms and t-conorms are used to define respectively an \emph{intersection} and
a \emph{union} of fuzzy sets. Let $X, Y$ be fuzzy sets, $\otimes$ be
a~t-norm and $\oplus$ be a corresponding t-conorm. Then the membership function
of an intersection $X\cap Y$ and a union $X\cup Y$ of fuzzy sets $X$ and $Y$ is defined as:

\begin{align*}
    (X\cap Y)(u) &= X(u) \otimes Y(u); \\
    (X\cup Y)(u) &= X(u) \oplus Y(u), \qquad \forall u\in U.
\end{align*}



\subsection{G\"odel Algebra}

Based on the selected t-norm, the \lfl{} package provides all the derived operations in a concise and extendable way. By calling the \code{algebra()} function with the name of the underlying t-norm, a named list of functions is obtained. The user may select from \code{"goedel"}, \code{"goguen"}, or \code{"lukasiewicz"} variant. For example, let us obtain the algebra based on the G\"odel t-norm:
%
\input{chunks/chunk-goedel.algebra}

As can be seen, the \code{algebra()} function returns a named list of the following functions:
\begin{itemize}
    \item \code{n}: (strict) negation defined as:
    $$
    \lnot\alpha = \begin{cases}
    1, & \text{if}\ \alpha = 0, \\
    0, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{ni}: involutive negation defined as:
    ${\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \min\{\alpha, \beta\};$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \max\{\alpha, \beta\};$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta, \\
    \beta, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}

\code{n} and \code{ni} are functions that accept a vector of numeric values as a single input and return a vector of negated values. \code{r} and \code{b} are two-argument functions, which compute the desired operation element-wisely. Similarly, \code{pt}, \code{pc}, \code{pi} and \code{ps} work element-wisely: they accept multiple vector arguments and compute the desired operation on first elements of the input vectors, then on second elements, etc. until the end of the vectors, which yields a vector of resulting values. The vectorized variants of these functions, i.e., \code{t}, \code{c}, \code{i} and \code{s} first merge all the input vector arguments and then calculate a single resulting value from it. See the example below for more information.
%
\input{chunks/chunk-goedel.algebra.examples}
%


\subsection{Goguen Algebra}

Goguen algebra provides operations based on the product t-norm.
%
\input{chunks/chunk-goguen.algebra}
%
It is defined as follows:
\begin{itemize}
    \item \code{n}: (strict) negation defined as:
    $$
    \lnot\alpha = \begin{cases}
    1, & \text{if}\ \alpha = 0, \\
    0, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{ni}: involutive negation defined as:
    ${\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \alpha\beta;$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \alpha + \beta - \alpha\beta;$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta, \\
    \frac{\beta}{\alpha}, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}



\subsection{\L{}ukasiewicz Algebra}

\input{chunks/chunk-lukasiewicz.algebra}
%
\L{}ukasiewicz algebra provides operations based on the \L{}ukasiewicz t-norm as follows.
\begin{itemize}
    \item \code{n}, \code{ni}: both negations are equally defined as:
    $\lnot\alpha = {\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \max\{0, \alpha + \beta - 1\};$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \min\{1, \alpha + \beta\};$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta, \\
    1 - \alpha + \beta, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}



\subsection{Handling of Missing Values}

The implementation of algebra functions in \pkg{lfl} takes a special care for missing values.
If \code{NA} appears as a value to some operation,
it is propagated to the result. That is, any operation with \code{NA} results in \code{NA}, by default.
This scheme of handling missing values is also known as \emph{Bochvar's internal logic} \citep{book:malinov}.

The \code{sobocinski()}, \code{kleene()} and \code{dragonfly()} functions modify the algebra to
handle the \code{NA} in a different way than default. Sobocinski's algebra simply ignores \code{NA} values
whereas Kleene's algebra treats \code{NA} as ``unknown value''. Dragonfly approach is a combination
of Sobocinski's and Bochvar's approach, which preserves the ordering $0 \le \texttt{NA} \le 1$.

In detail, the behaviour of the algebra modifiers is defined as follows: \todo{doplnit tabulky pro NA hodnoty}



\section*{Acknowledgments}

All acknowledgments (note the AE spelling) should be collected in this
unnumbered section before the references. It may contain the usual information
about funding and feedback from colleagues/reviewers/etc. Furthermore,
information such as relative contributions of the authors may be added here
(if any).


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{bibliography}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\section{More technical details} \label{app:technical}

Appendices can be included after the bibliography (with a page break). Each
section within the appendix should have a proper section title (rather than
just \emph{Appendix}).

For more technical style details, please check out JSS's style FAQ at
\url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
which includes the following topics:
\begin{itemize}
  \item Title vs.\ sentence case.
  \item Graphics formatting.
  \item Naming conventions.
  \item Turning JSS manuscripts into \proglang{R} package vignettes.
  \item Trouble shooting.
  \item Many other potentially helpful details\dots
\end{itemize}


\section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
\verb|\citealp| etc.\ (and never hard-coded). This commands yield different
formats of author-year citations and allow to include additional details (e.g.,
pages, chapters, \dots) in brackets. In case you are not familiar with these
commands see the JSS style FAQ for details.

Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
when acquiring the entries automatically from mixed online sources. However,
it is important that informations are complete and presented in a consistent
style to avoid confusions. JSS requires the following format.
\begin{itemize}
  \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
    be used in the references.
  \item Titles should be in title case.
  \item Journal titles should not be abbreviated and in title case.
  \item DOIs should be included where available.
  \item Software should be properly cited as well. For \proglang{R} packages
    \code{citation("pkgname")} typically provides a good starting point.
\end{itemize}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
