\documentclass[article]{jss}

\usepackage{thumbpdf,lmodern}
\usepackage{amsmath}

\newcommand{\lfl}{\pkg{lfl}}
\newcommand{\R}{\proglang{R}}
\newcommand{\todo}[1]{{\color{red} TODO: #1}}

\newcommand{\tgodel}{{T}_{\textrm{M}}}
\newcommand{\tluk}{{T}_{\textrm{\L}}}
\newcommand{\tgoguen}{{T}_{\textrm{P}}}
\newcommand{\sgodel}{{C}_{\textrm{M}}}
\newcommand{\sgoguen}{{C}_{\textrm{P}}}
\newcommand{\sluk}{{C}_{\textrm{\L}}}
\newcommand{\rgodel}{{R}_{\textrm{M}}}
\newcommand{\rgoguen}{{R}_{\textrm{P}}}
\newcommand{\rluk}{{R}_{\textrm{\L}}}
\newcommand{\bgodel}{{B}_{\textrm{M}}}
\newcommand{\bgoguen}{{B}_{\textrm{P}}}
\newcommand{\bluk}{{B}_{\textrm{\L}}}
\newcommand{\ngodel}{{N}_{\textrm{M}}}
\newcommand{\ngoguen}{{N}_{\textrm{P}}}
\newcommand{\nluk}{{N}_{\textrm{\L}}}




%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Achim Zeileis\\Universit\"at Innsbruck
   \And Second Author\\Plus Affiliation}
\Plainauthor{Achim Zeileis, Second Author}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\lfl{}: Linguistic Fuzzy Logic in \R{}}
\Plaintitle{lfl: Linguistic Fuzzy logic in R}
\Shorttitle{\lfl{}: Linguistic Fuzzy Logic in \R{}}

%% - \Abstract{} almost as usual
\Abstract{
  This paper presents an \R{} package that enables the use of linguistic
  fuzzy logic in data processing applications. The \lfl{} package provides tools for
  transformation of data into fuzzy sets representing linguistic expressions, mining
  of linguistic fuzzy association rules, perfoming an inference on fuzzy rule bases
  using the Perception-based logical deduction (PbLD), and computing compositions of
  fuzzy relations. The package also enables to use the Fuzzy rule-based ensemble,
  a tool for time series forecasting based on an ensemble of forecasts from several individual
  methods implemented in \R{}.
}


\Keywords{fuzzy sets, fuzzy natural logic, linguistic fuzzy logic, association rules, compositions of relations, \R{}}
\Plainkeywords{fuzzy sets, fuzzy natural logic, linguistic fuzzy logic, association rules, compositions of relations, R}


%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Achim Zeileis\\
  Journal of Statistical Software\\
  \emph{and}\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section{Introduction} \label{sec:intro}


The aim of this paper is to present a new package for the \R{} statistical environment
\citep{R2020} that enables the use of the linguistic fuzzy logic in data processing
applications. The package provides implemented tools for using the results of original work of
\cite{Novak08, Novak:PbLD, DvoStep:PbLD2015, StepBurda:FRBE_FSS}, and others, and it provides
executable routines that are not freely available anywhere else.

Indeed, there already exist several packages for \R{} that are focused on vagueness and fuzziness. For
instance, the \pkg{sets}
package \citep{setsPkg} introduces many basic operations on fuzzy sets, the \pkg{FuzzyNumbers}
package \citep{FuzzyNumbersPkg} provides classes and methods to deal with fuzzy numbers,
the \pkg{SAFD} package \citep{safdPkg} contains tools for elementary statistics on fuzzy data, and
the \pkg{fclust} \citep{fclustPkg} brings the fuzzy K-Means clustering technique to the environment of
the \R{} system.

The \lfl{} package described in this paper focuses on creation of systems based on fuzzy logic and their
usage in classification and prediction. A similar task is performed also by the \pkg{fugeR}
package \citep{fugeRPkg} that introduces an evolutionary algorithm for a construction of a fuzzy system from
a training data set, or by the \pkg{frbs} package \citep{frbsPkg} that provides many widely
accepted approaches for building the fuzzy systems, based on space partition, neural networks,
clustering, gradient descent, or genetic algorithms.

The algorithms provided by the \lfl{} package are tightly connected with the notion of
the \emph{fuzzy natural logic} (FNL), formerly also called the \emph{linguistic fuzzy logic} (LFL),
that was initially developed by \cite{Novak08}. Moreover, it covers some other closely related areas, for example fuzzy relational calculus  (\cite{BandlerKohout78,Belohlavek_book2002,Behounek:Compositions})  that includes the latest generalizations (\cite{Cao2017, Cao2017b}), the algebraic structures for partial fuzzy logics (\cite{BehounekNovak:IEEE15, BehounekDankova:IPMU16}), and the connection of both topics (\cite{Step_etal_Dragon_IJAR2019}).

\emph{Evaluative linguistic expression} -- a central notion of the fuzzy natural logic -- is the expression of the form
%
$$\langle \textrm{linguistic hedge}\rangle \langle \textrm{atomic expression}\rangle$$
%
that evaluates the ruoghly a position on the real line, for example, ``very small'', ``roughly medium'', or ``extremely big''. The atomic expression takes values usually from the triplet 
``small'', ``medium'', and ``big''and its vague information can be adjusted by the used linguistic hedge (such as ``very'', ``extremely'', ``roughly'' or ``more or less'').
The particular \emph{fuzzy sets} modeling the semantics of the evaluative linguistic expressions including the justification can be found in \cite{Novak08}, see also Figure~\ref{fig_dee}. A mathematical framework for manipulation and reasoning with
such linguistic expressions is provided in a specific inference method called  \emph{Perception-based
Logical Deduction} (PbLD), that was tailored to the above-mentioned expressions, see \cite{Dvovrak2004formal,Novak:PbLD, Dvorak_etal:RedundancyFSS}.



Unlike the traditional approach of \cite{MamdaniAssilian75}, who build the
rule base as a disjunction of conjunctions of antecedents and consequents, the PbLD approach is
closer to the implicative approach (\cite{Jones2009}, \cite{Step_etal_continuity}) since it employes genuine residuated implications to connect antecedents and consequents. However, it does not aggregate them conjunctively and it considers the rule base as a list of fuzzy rules from which only single or a very few are fired. The function choosing the particular rules to be fired is calle \emph{perception} and it takes into account the  \emph{specificity} of the antecedents of the rules.
For instance, the antecedent ``age is very small'' is more specific than the antecedent ``age is small'', see the inclusion of the respective fuzzy sets in Figure~\ref{fig_dee}. 
In PbLD, rules with more specific antecedents take the precedence over the rules with more general antecedents, assuming that both of them fire in the same degree. That enables, e.g., to employ big discontinuous jumps in the control actions according to the needs of the particular
application. We refer to \cite{Novak:PbLD,DvoStep:PbLD2015} for all the details on PbLD.



Another software, dealing with the
similar topic as the \lfl{} package, is \emph{Linguistic Fuzzy Logic Controller} (LFLC) by \cite{dvo:lflc}.
However, it is not connected with the \R{} statistical environment and unlike \lfl{} package, LFLC is not fully free nor an open-source.For an exhaustive study on existing software implementations of fuzzy methods we refere to the recent survey \cite{AlcalAlonso:fuzzySW_IEEETFS16}.


The \lfl{} package also provides functions for searching for fuzzy association rules (\cite{srikant95}). Together
with PbLD, they can be used as a machine learning tool for classification or regression problems, see \cite{KupkaRusnok:Regression_ESWA_INS17}.
The package also includes the \emph{Fuzzy Rule-based Ensemble}, a tool for time series forecasting
\citep{StepBurda:FRBE_FSS}, which is built on top of the fuzzy association rules search algorithm and PbLD.

Alternatively to machine learning, classification tasks may be solved based on human expert knowledge
by using the technique of \emph{compositions of fuzzy relations}, see
\cite{Step2014,Cao2017b,Cao2017}.




\subsection{Overview of the Paper}

The rest of the paper is organized as follows. \todo{doplnit}




\subsection[How to Obtain the lfl Package]{How to Obtain the \lfl{} Package} \label{sec:obtaining}

To obtain the \lfl{} package, a working instance of the \R{} statistical environment has to be installed
first and then the
%
\input{chunks/chunk-cran.lfl}
%
command automatically downloads the latest stable version of the \lfl{} package from CRAN together with all
its dependencies, compiles, and installs it. The \lfl{} package works on all platforms supported by
the \R{} software including Microsoft Windows, GNU/Linux, and MacOS. Alternatively, the development
version may be installed directly from GitHub by issuing following commands within the R session:
%
\input{chunks/chunk-git.lfl}
%
After the installation is successful, the following command
causes loading of the package into the working space so that the user can start using it:
%
\input{chunks/chunk-load.lfl}
%



%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)



\section{Fuzzy Logic and Fuzzy Sets} \label{sec:fuzzysets}


Fuzzy sets were proposed by \cite{Zadeh65} as a generalization of classical sets in such a way that elements may belong to them in a certain degree, most typically from the unit interval $[0,1]$. Formally, a \emph{fuzzy set} $A$ from a universe $U$ is defined as a mapping $A: U \to [0, 1]$ and $A(u)$ is called \emph{membership degree of the element $u$ in the fuzzy set $A$}. As $\{0,1\} \subset [0,1]$ and $A(u)= 0$ and $A(u) = 1$ can be taken as $u \notin A$ and $u\in A$, respectively, a classical set is a special case of a fuzzy set. A \emph{cardinality} $|A|$ of a fuzzy set $A$ on a finite universe $U$ can be defined as
the sum of the membership degrees \citep{novak1999}:
%
$$|A| = \sum_{\forall u\in U} A(u).$$



In order to extend the intersection and union of classical (binary) sets to $[0,1]$ membership degrees, most commonly triangular norms and conorms, respectively. 

\emph{Triangular norm (t-norm)} is a function $\otimes: [0,1] \times [0,1] \to [0,1]$,
which is associative, commutative, monotone increasing (in both
places), and with the neutral element 1, i.e., with $\alpha \otimes 1 = \alpha$ for each $\alpha \in [0, 1]$. The annihilating effect of the element 0 can be easily derived: $\alpha \otimes 0 = 0$ for each $\alpha \in [0, 1]$.

In fuzzy logic\footnote{Specific multiple-valued logic for modeling the vagueness phenomenon or gradedness.}, a t-norm $T$ is used to model the \emph{conjunction}. \emph{Triangular conorm (t-conorm)} is a function $\oplus: [0,1] \times [0,1] \to [0,1]$ that possesses the same properties as t-norm with the only difference that the neutral element is here 0, i.e., $\alpha \oplus 0 = \alpha$ for any $\alpha \in [0,1]$. Consequently, we obtain $\alpha \oplus 1 = 1$ for any $\alpha \in [0,1]$.
  
Analogously to the case of classical logic and classical set theory, also here we derive the \emph{intersection} and the \emph{union} of fuzzy sets from the above introduced logical operations. Let $A, B$ be fuzzy sets on $U$. Then the membership degree of an element $u\in U$ to their intersection
$A \cap B$ and their union $A\cup B$ is defined as follows:
\begin{align*}
    (A\cap B)(u) &= A(u) \otimes B(u) \ , \\
    (A\cup B)(u) &= A(u) \oplus B(u) \ .
\end{align*}


For any left-continuous t-norm $\otimes$, there is a unique binary operation $\Rightarrow$
called the \emph{residuum} (\emph{residuated implication}) of the t-norm $\otimes$ such that the following adjunction property holds
%
$$\gamma \otimes \alpha \le \beta\quad\textrm{if and only if}\quad \gamma \le \alpha \Rightarrow \beta \ . $$
%
Residuated implications possess special position among other fuzzy implications (\cite{Baczynski_Jayaram_2008c}) in capturing the multiple-valued modus ponens property. 

Stemming from a left-continuous t-norm $\otimes$, we may construct so-called \emph{residuated lattice} $\langle [0,1], \wedge, \vee, \otimes , \Rightarrow , 0 , 1 \rangle$ that is the underlying algebraic structure of truth-values of the respective fuzzy logic. Note, that the structure has two conjunctions, the \emph{strong} conjunction $\otimes$ and the \emph{weak} conjuction $\wedge$ that is present in any residuated lattice. Furthermore, we may define additional logical connectives, for instance, the \emph{residual negation} $\lnot$
and \emph{biresiduum}  $\Leftrightarrow$ that models the multiple-valued equivalence:
%
\begin{align*}
\lnot\alpha &= \alpha \Rightarrow 0 \ , \\
\alpha \Leftrightarrow \beta &= (\alpha \Rightarrow \beta) \otimes (\beta \Rightarrow \alpha)\ .
\end{align*}

For the particular \L ukasiewicz t-norm, the residual negation $\lnot$ leads to the  \emph{involutive} negation $\neg \alpha = 1-\alpha$ that obeys the law of double negation $\lnot\lnot\alpha = \alpha$. Let us denote such an involutive negation by $\sim$. The involutive negation can be used in deriving the dual t-conorm from a given t-norm (and vice-versa) or, in other words, in deriving the \emph{strong disjunction} from a given strong conjunction:
$$\alpha\oplus\beta = {\sim}({\sim}\alpha \otimes {\sim}\beta)\ .$$

Therefore, $\sim$ is an important unary connective not only for the \L ukasiewicz algebra but for all residuated lattices and we may freely extend such structures by the involutive negation for further use: $\langle [0,1], \wedge, \vee, \otimes , \Rightarrow, \sim 0 , 1 \rangle$. It does not mean that $\neg$ is not at disposal, it is always present via the definition recalled above and we have in general two negations that, in the case of the \L ukasiewicz algebra, coincide. The fact that the weak conjunction $\wedge$ and the weak disjunction $\vee$ are the lattice operations \emph{meet} (\emph{infimum})  $\land$ and the \emph{join} (\emph{supremum})  $\lor$ needs not further explanation.

{\color{blue}M: tuto uvodni sekci 2 az do 2.1 povazuji za cistou, koukni na to, podle mne hotovo az na spell check, jinak nic nechybi, ani relevantni reference}


\subsection{G\"odel Algebra}



Based on the selected t-norm, the \lfl{} package provides all the derived operations in a concise and extendable way. By calling the \code{algebra()} function with the name of the underlying t-norm, a named list of functions is obtained. The user may select from \code{"goedel"}, \code{"goguen"}, or \code{"lukasiewicz"} variant calling the respective G\"odel, Goguen (also often called \emph{product}), or the already above-mentioned \L ukasiewicz residuated lattices of operations. 

For example, the algebra based on the G\"odel t-norm, that is the standard minimum $\otimes = \wedge$, is obtained as follows:
%
\input{chunks/chunk-goedel.algebra}

As can be seen, the \code{algebra()} function returns a named list of the following functions:
\begin{itemize}
    \item \code{n}: (strict) negation defined as:
    $$
    \lnot\alpha = \begin{cases}
    1, & \text{if}\ \alpha = 0 \ , \\
    0, & \text{otherwise} \ ;
    \end{cases}
    $$
    \item \code{ni}: involutive negation defined as:
    ${\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \min\{\alpha, \beta\};$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \max\{\alpha, \beta\};$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta \ , \\
    \beta, & \text{otherwise}\ ;
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}


Functions \code{n} and \code{ni} accept a vector of numeric values as a single input and return a vector of the negated values. Two-argument functions \code{r} and \code{b} compute the desired operation element-wisely. Similarly, \code{pt}, \code{pc}, \code{pi}, and \code{ps} work element-wisely: they accept a vector of multiple arguments and compute the outputs of the desired operation on first elements of the input vectors, then on second elements, etc. until the end of the vectors is reached, which yields a vector of the resulting values. The vectorized variants of these functions, i.e., \code{t}, \code{c}, \code{i}, and \code{s} first merge\footnote{By merging here, we mean application of the chosen operation on all components of the given vector to obtain a single value.} all the input vector arguments and then calculate a single resulting value from them. See the example below for more information.
%
\input{chunks/chunk-goedel.algebra.examples}
%

Note that as the strong and weak conjunction coincide in the G\"{o}del algebra as well as the strong and weak disjunction coincide, also the following holds for the  functions in the \lfl{} R-package: \code{t} = \code{i}, \code{pt} = \code{pi}, \code{c} = \code{s}, and \code{pc} = \code{ps}.




\subsection{Goguen Algebra}



Goguen algebra is also often called the product algebra to emphasize that its central point -- the strong conjunction -- is nothing else but the standard product (multiplication) operation. Therefore, $\otimes = \cdot$ is also often called the product t-norm.
%
\input{chunks/chunk-goguen.algebra}
%
It is defined as follows:
\begin{itemize}
    \item \code{n}: (strict) negation defined as:
    $$
    \lnot\alpha = \begin{cases}
    1, & \text{if}\ \alpha = 0\ , \\
    0, & \text{otherwise}
\ ;    \end{cases}
    $$
    \item \code{ni}: involutive negation defined as:
    ${\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \alpha\beta;$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \alpha + \beta - \alpha\beta;$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta\ , \\
    \frac{\beta}{\alpha}, & \text{otherwise}\ ;
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}

{\color{blue}M: sekce 2.1 a 2.2 za mne ciste}

\subsection{\L{}ukasiewicz Algebra}

The last implemented algebra is the \L{}ukasiewicz algebra that stems from the seminal work on 3-valued logic by Polish logician Jan \L ukasiewicz, see \cite{lukasiewicz19673}. Note, that \L ukasiewicz algebra forms so-called MV algebra (\cite{Chang:MV_1958}) that is the best generalization of the classical Boolean algebra. The implementation is provided as follows:

\input{chunks/chunk-lukasiewicz.algebra}
%
The particular functions are defined as follows:

\begin{itemize}
    \item \code{n}, \code{ni}: both negations are equally defined as:
    $\lnot\alpha = {\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \max\{0, \alpha + \beta - 1\};$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \min\{1, \alpha + \beta\};$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta, \\
    1 - \alpha + \beta, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}



\subsection{Partial Fuzzy Set Theory -- Handling of Undefined and Missing Values}

{\color{red} tak k tomuto se musime sejit a pokecta, tady si dovedu predstavit tak hrozne ruzne styly, jak to popsat, ze si musime rict, jak ma ten clanek vlastne vypadat, at ti to neborim pod rukama}

The implementation of algebra functions in \pkg{lfl} takes a special care for missing values.
If \code{NA} appears as a value to some operation,
it is propagated to the result. That is, any operation with \code{NA} results in \code{NA}, by default.
This scheme of handling missing values is also known as \emph{Bochvar's internal logic} \citep{book:malinov}.

The \code{sobocinski()}, \code{kleene()}, \code{nelson()}, \code{lowerEst()} and \code{dragonfly()} functions modify the algebra to
handle \code{NA}s in a different way than default. Sobocinski's algebra simply ignores \code{NA} values
whereas Kleene's algebra treats \code{NA} as ``unknown value''. Dragonfly approach is a combination
of Sobocinski's and Bochvar's approach, which preserves the ordering $0 \le \code{NA} \le 1$.

In detail, the behaviour of the algebra modifiers is defined as follows: \todo{prosim o kontrolu tabulek -- pokud je v nich chyba, znamena to, ze mam chybu i ve zdrojacich v R}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of residual negation}
    \label{tab:my_label}
    \begin{tabular}{c|cccccc}
        $\lnot$     & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ \\
        \code{NA} & \code{NA} & $0$         & \code{NA} & $1$         & \code{NA} & $0$
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of involutive negation}
    \label{tab:my_label}
    \begin{tabular}{c|cccccc}
        $\sim$     & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA}
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of conjunctive operations}
    \label{tab:my_label}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\otimes$, $\land$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $0$         & $0$         & $0$         & $0$         & $0$         \\
        $\alpha$    & \code{NA} & \code{NA} & $\alpha$    & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $1$         & \code{NA} & \code{NA} & $1$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $0$         & \code{NA} & $0$         & $0$         & $0$         & $0$         & $0$         \\
        \code{NA} & $\beta$     & \code{NA} & $\beta$     & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $1$         & \code{NA} & $1$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} 
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of disjunctive operations}
    \label{tab:my_label}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\oplus$, $\lor$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $0$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $\alpha$    & \code{NA} & \code{NA} & $\alpha$    & \code{NA} & \code{NA} & $\alpha$    & $\alpha$    \\
        $1$         & \code{NA} & \code{NA} & $1$         & $1$         & $1$         & $1$         & $1$         \\
        \code{NA} & $0$         & \code{NA} & $0$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $\beta$     & \code{NA} & $\beta$     & \code{NA} & \code{NA} & $\beta$     & $\beta$     \\
        \code{NA} & $1$         & \code{NA} & $1$         & $1$         & $1$         & $1$         & $1$         \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} 
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of residuum}
    \label{tab:my_label}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\Rightarrow$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $1$           & $1$         & $1$         & $1$         & $1$         \\
        $\alpha$    & \code{NA} & \code{NA} & $\lnot\alpha$ & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $1$         & \code{NA} & \code{NA} & $0$           & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $0$         & \code{NA} & $0$           & \code{NA} & $1$         & \code{NA} & $0$         \\
        \code{NA} & $\beta$     & \code{NA} & $\beta$       & \code{NA} & \code{NA} & $\beta$     & $\beta$     \\
        \code{NA} & $1$         & \code{NA} & $1$           & $1$         & $1$         & $1$         & $1$         \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA}   & \code{NA} & $1$         & $1$         & \code{NA} 
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of bi-residuum}
    \label{tab:my_label}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\Leftrightarrow$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $0$         & \code{NA} & $1$         & ? & ? \\
        $\alpha$    & \code{NA} & \code{NA} & $0$         & \code{NA} & \code{NA} & ? & ? \\
        $1$         & \code{NA} & \code{NA} & $0$         & \code{NA} & \code{NA} & ? & ? \\
        \code{NA} & $0$         & \code{NA} & $0$         & \code{NA} & $1$         & ? & ? \\
        \code{NA} & $\beta$     & \code{NA} & $0$         & \code{NA} & \code{NA} & ? & ? \\
        \code{NA} & $1$         & \code{NA} & $0$         & \code{NA} & \code{NA} & ? & ? \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & $1$         & ? & ? 
    \end{tabular}
\end{table}

By default, the functions in the structure that is obtained by calling the \code{algebra()} function simply propagate \code{NA} to the output. If some other handling of missing values is required, it can be done as follows. Firstly, the underlying algebra (G\"odel, Goguen or \L{}ukasiewicz) is created and then modified by applying one of the \code{sobocinski()}, \code{kleene()}, \code{nelson}, \code{dragonfly}, \code{lowerEst} functions on it -- see the example:
%
\input{chunks/chunk-na.algebra}



\section{Evaluative Linguistic Expressions}



\begin{figure}[htbp]
	\centering
	\includegraphics[width=120mm]{DEE.png}
	\caption{Fuzzy sets representing some particular evaluative linguistive expressions. Specific defuzzification DEE is charted too.}\label{fig_dee}
\end{figure}

\todo{tady bude blabla o lingvistickych vyrazech}


\subsection{Linguistic Context}

In order to work with linguistic expressions in the \pkg{lfl} package, an appropriate \emph{linguistic context} must be defined firstly. A context describes a range of allowed values. For that, only the borders of the interval, i.e. minimum and maximum, are usually needed, but in \pkg{lfl}, the contexts carry an additional information of the position of other typical values, which in turn define the set of allowed atomic expression to use.

Currently, four different contexts are supported in \pkg{lfl} that determine the types of possible linguistic expressions. The \emph{unilateral} or \emph{bilateral} context is allowed in the variant of \emph{trichotomy} or \emph{pentachotomy}. Trichotomy distinguishes the three points in the interval: the lowest value, the highest value, and the center. Pentachotomy adds the lower center and the upper center to them. As opposite to unilateral, the bilateral context handles explicitly the negative values. That is, bilateral context expects some middle point, the origin (usually 0), around which the positive and negative values are placed.

The linguistic context is created by calling a function \code{ctx3()}, \code{ctx5()}, \code{ctx3bilat()} or \code{ctx5bilat()}. The type of the context determines the allowed atomic expressions as follows:
%
\begin{itemize}
    \item \code{ctx3(low, center, high)}: the unilateral trichotomy that enables the atomic expressions: \emph{small, medium, big}; 
    \item \code{ctx5(low, lowerCenter, center, upperCenter, high)}: the unilateral pentacho\-to\-my that enables the atomic expressions: \emph{small, lower medium, medium, upper medium, big}; 
    \item \code{ctx3bilat(negMax, negCenter, origin, center, max)}: the bilateral trichotomy that enables the atomic expressions: \emph{negative big, negative medium, negative small, zero, small, medium, big};
    \item \code{ctx5bilat(negMax, negUpperCenter, negCenter, negLowerCenter, origin, low\-erCenter, center, upperCenter, max)}: the bilateral pentachotomy that enables the atomic expressions: \emph{negative big, negative upper medium, negative medium, negative lower medium, negative small, zero, small, lower medium, medium, upper medium, big}.
\end{itemize}
%
The arguments of context creator functions have sensible defaults and need not be therefore explicitly stated in all cases:
%
\input{chunks/chunk-ctx}
%
Alternatively, the context may be automatically determined from data by calling the \code{minmax()} function, which creates the selected type of the context based on the minimum and maximum value found in data:
%
\input{chunks/chunk-minmax}
%
The \code{minmax()} function may be forced not to guess some values by specifying them explicitly as additional arguments:
%
\input{chunks/chunk-minmax2}



\subsection{Evaluative Linguistic Expressions}

A linguistic expression consists of an atomic expression and a hedge. The atomic expression such as \emph{small}, \emph{medium} or \emph{big} is accordingly to the theory of evaluative linguistic expressions discussed above modelled with the \code{horizon()} function. Horizon of the atomic expression is a triangular function which represents basic limits of what humans treat as small, medium or big.
%
\input{chunks/chunk-horizon}
%
A view of horizons of the three basic atomic expressions (\emph{small}, \emph{medium} and \emph{big}) can be found in Figure~\ref{fig:horizon}.

\begin{figure}
    \centering
    \includegraphics{figure/horizon-1.pdf}
    \caption{Horizons for the atomic expressions \emph{small}, \emph{medium} and \emph{big} in the unilateral trichotomy (\code{ctx3()})}
    \label{fig:horizon}
\end{figure}

A hedge applied to the horizon produces a final linguistic expression. In \pkg{lfl}, the \code{hedge()} function creates a modifier function, which, combined with the horizon, produces a final linguistic expression. For instance, the function that represents the ``very small'' expression can be obtained as follows:
%
\input{chunks/chunk-hedge}
%
Such approach gives the user a detailed control of the creation of a linguistic expression, which may be useful for experimenting with novel expressions. However, it may be tedious to manually create the functions for a routine use. Therefore, the \code{lingexpr()} function provides a shortcut for creation of the pre-defined expressions:
%
\input{chunks/chunk-lingexpr}

Note that for a linguistic expression consisting of an atomic expression only, an empty hedge has to be used:
%
\input{chunks/chunk-emptyhedge1}
%
or equivalently:
%
\input{chunks/chunk-emptyhedge2}

\begin{figure}
    \centering
    \includegraphics{figure/lingexpr1-1.pdf}
    \includegraphics{figure/lingexpr2-1.pdf}
    \includegraphics{figure/lingexpr3-1.pdf}
    \caption{}
    \label{fig:lingexpr}
\end{figure}

\section{Compositions of Fuzzy Relations}



\section{Fuzzy Association Rules}



\section{Perception-based Logical Deduction}



\section{Fuzzy Rule-based Ensemble for Time Series Prediction}



\section{Conclusion}



\section*{Acknowledgments}

Partial support of Czech Science Foundation through the grant 20-07851S is gratefully announced.


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{bibliography}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\section{More technical details} \label{app:technical}

Appendices can be included after the bibliography (with a page break). Each
section within the appendix should have a proper section title (rather than
just \emph{Appendix}).

For more technical style details, please check out JSS's style FAQ at
\url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
which includes the following topics:
\begin{itemize}
  \item Title vs.\ sentence case.
  \item Graphics formatting.
  \item Naming conventions.
  \item Turning JSS manuscripts into \proglang{R} package vignettes.
  \item Trouble shooting.
  \item Many other potentially helpful details\dots
\end{itemize}


\section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
\verb|\citealp| etc.\ (and never hard-coded). This commands yield different
formats of author-year citations and allow to include additional details (e.g.,
pages, chapters, \dots) in brackets. In case you are not familiar with these
commands see the JSS style FAQ for details.

Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
when acquiring the entries automatically from mixed online sources. However,
it is important that informations are complete and presented in a consistent
style to avoid confusions. JSS requires the following format.
\begin{itemize}
  \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
    be used in the references.
  \item Titles should be in title case.
  \item Journal titles should not be abbreviated and in title case.
  \item DOIs should be included where available.
  \item Software should be properly cited as well. For \proglang{R} packages
    \code{citation("pkgname")} typically provides a good starting point.
\end{itemize}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
