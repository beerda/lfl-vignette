\documentclass[article]{jss}

\usepackage{thumbpdf,lmodern}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\lfl}{\pkg{lfl}}
\newcommand{\R}{\proglang{R}}
\newcommand{\todo}[1]{{\color{red} TODO: #1}}
\newcommand{\michal}[1]{{\color{green} MB: #1}}

\newcommand{\tgodel}{{T}_{\textrm{M}}}
\newcommand{\tluk}{{T}_{\textrm{\L}}}
\newcommand{\tgoguen}{{T}_{\textrm{P}}}
\newcommand{\sgodel}{{C}_{\textrm{M}}}
\newcommand{\sgoguen}{{C}_{\textrm{P}}}
\newcommand{\sluk}{{C}_{\textrm{\L}}}
\newcommand{\rgodel}{{R}_{\textrm{M}}}
\newcommand{\rgoguen}{{R}_{\textrm{P}}}
\newcommand{\rluk}{{R}_{\textrm{\L}}}
\newcommand{\bgodel}{{B}_{\textrm{M}}}
\newcommand{\bgoguen}{{B}_{\textrm{P}}}
\newcommand{\bluk}{{B}_{\textrm{\L}}}
\newcommand{\ngodel}{{N}_{\textrm{M}}}
\newcommand{\ngoguen}{{N}_{\textrm{P}}}
\newcommand{\nluk}{{N}_{\textrm{\L}}}

\newcommand{\squ}{\mathop{\scriptstyle \square}\nolimits}
\newcommand{\dhd}{\mathop{\triangledown}\nolimits}

\newcommand{\RSE}{\mathop{(R\circ S^{\backprime}E)}\nolimits}

\def\lh{\mathop{\le_{\rm H} }\nolimits}


\newcommand{\T}{\text{\it T}}
\newcommand{\LD}{\text{\it LD}}
\newcommand{\Topic}{\text{\it Topic}}
\newcommand{\ExTop}{\text{\it ExTopic}}
\newcommand{\Focus}{\text{\it Focus}}
\newcommand{\LPerc}{\text{\it P}}
\newcommand{\lingterm}[1]{\text{\sf #1}}
\newcommand{\defeq}{\mathrel{:=}\,}
\newcommand{\LpredC}[2]{#1\text{ is }#2}
\newcommand{\Lpred}[2]{#2\ #1}
\newcommand{\IFRule}[4]{\lingterm{IF } \LpredC{#1}{#2}
\lingterm{ THEN } \LpredC{#3}{#4}}

\newcommand{\IFRuleMore}[6]{\lingterm{IF } \LpredC{#1}{#2} \lingterm{ AND
}\cdots \lingterm{ AND }\LpredC{#3}{#4} \lingterm{ THEN }
\LpredC{#5}{#6}}


\newcommand{\mlingterm}[1]{\langle\text{#1}\rangle}
\newcommand{\Core}{\mathop{\rm Core}\nolimits}
\newcommand{\Supp}{\mathop{\rm Supp}\nolimits}
\newcommand{\cov}{\mathop{\rm cov_{\LD}}\nolimits}
\newcommand{\covr}{\mathop{\rm cov_{\LD'}}\nolimits}
\newcommand{\red}{\mathop{\rm reduct_{\LD}^{\LD'}}\nolimits}

\def\Sm{\mathop{\rm Sm}\nolimits}
\def\Me{\mathop{\rm Me}\nolimits}
\def\Bi{\mathop{\rm Bi}\nolimits}
\def\lh{\mathop{\le_{\rm H} }\nolimits}
\def\llp{\mathop{\le_{x_0} }\nolimits}
%\def\llp{\mathop{\le_{(u_0,w)} }\nolimits}
\def\str {\mathop{<_{\rm LE} }\nolimits}
\def\lle{\mathop{\le_{\rm LE} }\nolimits}
\def\nlle{\mathop{\nleq_{\rm LE} }\nolimits}
\def\plle{\mathop{\parallel_{\rm LE} }\nolimits}
\def\lle{\mathop{\le_{\rm LE} }\nolimits}
\def\les{\mathop{<_{\rm LE} }\nolimits}

%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Achim Zeileis\\Universit\"at Innsbruck
   \And Second Author\\Plus Affiliation}
\Plainauthor{Achim Zeileis, Second Author}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\lfl{}: Linguistic Fuzzy Logic in \R{}}
\Plaintitle{lfl: Linguistic Fuzzy logic in R}
\Shorttitle{\lfl{}: Linguistic Fuzzy Logic in \R{}}

%% - \Abstract{} almost as usual
\Abstract{
  This paper presents an \R{} package that enables the use of linguistic
  fuzzy logic in data processing applications. The \lfl{} package provides tools for
  transformation of data into fuzzy sets representing linguistic expressions, mining
  of linguistic fuzzy association rules, performing an inference on fuzzy rule bases
  using the Perception-based logical deduction (PbLD), and computing compositions of
  fuzzy relations. The package also enables to use the Fuzzy rule-based ensemble,
  a tool for time series forecasting based on an ensemble of forecasts from several individual
  methods implemented in \R{}.
}


\Keywords{fuzzy sets, fuzzy natural logic, linguistic fuzzy logic, association rules, compositions of relations, \R{}}
\Plainkeywords{fuzzy sets, fuzzy natural logic, linguistic fuzzy logic, association rules, compositions of relations, R}


%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Achim Zeileis\\
  Journal of Statistical Software\\
  \emph{and}\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section{Introduction} \label{sec:intro}


The aim of this paper is to present a new package for the \R{} statistical environment
\citep{R2020} that enables the use of the linguistic fuzzy logic in data processing
applications. The package provides implemented tools for using the results of original work of
\cite{Novak08, Novak:PbLD, DvoStep:PbLD2015, StepBurda:FRBE_FSS}, and others, and it provides
executable routines that are not freely available anywhere else.

Indeed, there already exist several packages for \R{} that are focused on vagueness and fuzziness. For
instance, the \pkg{sets}
package \citep{setsPkg} introduces many basic operations on fuzzy sets, the \pkg{FuzzyNumbers}
package \citep{FuzzyNumbersPkg} provides classes and methods to deal with fuzzy numbers,
the \pkg{SAFD} package \citep{safdPkg} contains tools for elementary statistics on fuzzy data, and
the \pkg{fclust} \citep{fclustPkg} brings the fuzzy K-Means clustering technique to the environment of
the \R{} system.

The \lfl{} package described in this paper focuses on creation of systems based on fuzzy logic and their
usage in classification and prediction. A similar task is performed also by the \pkg{fugeR}
package \citep{fugeRPkg} that introduces an evolutionary algorithm for a construction of a fuzzy system from
a training data set, or by the \pkg{frbs} package \citep{frbsPkg} that provides many widely
accepted approaches for building the fuzzy systems, based on space partition, neural networks,
clustering, gradient descent, or genetic algorithms.

The algorithms provided by the \lfl{} package are tightly connected with the notion of
the \emph{fuzzy natural logic} (FNL), formerly also called the \emph{linguistic fuzzy logic} (LFL),
that was initially developed by \cite{Novak08}. Moreover, it covers some other closely related areas, for example fuzzy relational calculus  (\cite{BandlerKohout78,Belohlavek_book2002,Behounek:Compositions})  that includes the latest generalizations (\cite{Cao2017, Cao2017b}), the algebraic structures for partial fuzzy logics (\cite{BehounekNovak:IEEE15, BehounekDankova:IPMU16}), and the connection of both topics (\cite{Step_etal_Dragon_IJAR2019}).

\emph{Evaluative linguistic expression} -- a central notion of the fuzzy natural logic -- is the expression of the form
%
$$\langle \textrm{linguistic hedge}\rangle \langle \textrm{atomic expression}\rangle$$
%
that evaluates the roughly a position on the real line, for example, ``very small'', ``roughly medium'', or ``extremely big''. The atomic expression takes values usually from the triplet 
``small'', ``medium'', and ``big''and its vague information can be adjusted by the used linguistic hedge (such as ``very'', ``extremely'', ``roughly'' or ``more or less'').
The particular \emph{fuzzy sets} modeling the semantics of the evaluative linguistic expressions including the justification can be found in \cite{Novak08}, see also Figure~\ref{fig_dee}. A mathematical framework for manipulation and reasoning with
such linguistic expressions is provided in a specific inference method called  \emph{Perception-based
Logical Deduction} (PbLD), that was tailored to the above-mentioned expressions, see \cite{Dvovrak2004formal,Novak:PbLD, Dvorak_etal:RedundancyFSS}.



Unlike the traditional approach of \cite{MamdaniAssilian75}, who build the
rule base as a disjunction of conjunctions of antecedents and consequents, the PbLD approach is
closer to the implicative approach (\cite{Jones2009}, \cite{Step_etal_continuity}) since it employes genuine residuated implications to connect antecedents and consequents. However, it does not aggregate them conjunctively and it considers the rule base as a list of fuzzy rules from which only single or a very few are fired. The function choosing the particular rules to be fired is calle \emph{perception} and it takes into account the  \emph{specificity} of the antecedents of the rules.
For instance, the antecedent ``age is very small'' is more specific than the antecedent ``age is small'', see the inclusion of the respective fuzzy sets in Figure~\ref{fig_dee}. 
In PbLD, rules with more specific antecedents take the precedence over the rules with more general antecedents, assuming that both of them fire in the same degree. That enables, e.g., to employ big discontinuous jumps in the control actions according to the needs of the particular
application. We refer to \cite{Novak:PbLD,DvoStep:PbLD2015} for all the details on PbLD.



Another software, dealing with the
similar topic as the \lfl{} package, is \emph{Linguistic Fuzzy Logic Controller} (LFLC) by \cite{dvo:lflc}.
However, it is not connected with the \R{} statistical environment and unlike \lfl{} package, LFLC is not fully free nor an open-source.For an exhaustive study on existing software implementations of fuzzy methods we refer to the recent survey \cite{AlcalAlonso:fuzzySW_IEEETFS16}.


The \lfl{} package also provides functions for searching for fuzzy association rules (\cite{srikant95}). Together
with PbLD, they can be used as a machine learning tool for classification or regression problems, see \cite{KupkaRusnok:Regression_ESWA_INS17}.
The package also includes the \emph{Fuzzy Rule-based Ensemble}, a tool for time series forecasting
\citep{StepBurda:FRBE_FSS}, which is built on top of the fuzzy association rules search algorithm and PbLD.

Alternatively to machine learning, classification tasks may be solved based on human expert knowledge
by using the technique of \emph{compositions of fuzzy relations}, see
\cite{Step2014,Cao2017b,Cao2017}.




\subsection{Overview of the Paper}

The rest of the paper is organized as follows. \todo{doplnit}




\subsection[How to Obtain the lfl Package]{How to Obtain the \lfl{} Package} \label{sec:obtaining}

To obtain the \lfl{} package, a working instance of the \R{} statistical environment has to be installed
first and then the
%
\input{chunks/chunk-cran.lfl}
%
command automatically downloads the latest stable version of the \lfl{} package from CRAN together with all
its dependencies, compiles, and installs it. The \lfl{} package works on all platforms supported by
the \R{} software including Microsoft Windows, GNU/Linux, and MacOS. Alternatively, the development
version may be installed directly from GitHub by issuing following commands within the R session:
%
\input{chunks/chunk-git.lfl}
%
After the installation is successful, the following command
causes loading of the package into the working space so that the user can start using it:
%
\input{chunks/chunk-load.lfl}
%



%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)



\section{Fuzzy Logic and Fuzzy Sets} \label{sec:fuzzysets}


Fuzzy sets were proposed by \cite{Zadeh65} as a generalization of classical sets in such a way that elements may belong to them in a certain degree, most typically from the unit interval $[0,1]$. Formally, a \emph{fuzzy set} $A$ from a universe $U$ (denoted by $A\in\mathcal{F}(U)$) is defined as a mapping $A: U \to [0, 1]$ and $A(u)$ is called \emph{membership degree of the element $u$ in the fuzzy set $A$}. As $\{0,1\} \subset [0,1]$ and $A(u)= 0$ and $A(u) = 1$ can be taken as $u \notin A$ and $u\in A$, respectively, a classical set is a special case of a fuzzy set. A \emph{cardinality} $|A|$ of a fuzzy set $A$ on a finite universe $U$ can be defined as
the sum of the membership degrees \citep{novak1999}:
%
$$|A| = \sum_{\forall u\in U} A(u).$$



In order to extend the intersection and union of classical (binary) sets to $[0,1]$ membership degrees, most commonly triangular norms and conorms, respectively. 

\emph{Triangular norm (t-norm)} is a function $\otimes: [0,1] \times [0,1] \to [0,1]$,
which is associative, commutative, monotone increasing (in both
places), and with the neutral element 1, i.e., with $\alpha \otimes 1 = \alpha$ for each $\alpha \in [0, 1]$. The annihilating effect of the element 0 can be easily derived: $\alpha \otimes 0 = 0$ for each $\alpha \in [0, 1]$.

In fuzzy logic\footnote{Specific multiple-valued logic for modeling the vagueness phenomenon or gradedness.}, a t-norm $T$ is used to model the \emph{conjunction}. \emph{Triangular conorm (t-conorm)} is a function $\oplus: [0,1] \times [0,1] \to [0,1]$ that possesses the same properties as t-norm with the only difference that the neutral element is here 0, i.e., $\alpha \oplus 0 = \alpha$ for any $\alpha \in [0,1]$. Consequently, we obtain $\alpha \oplus 1 = 1$ for any $\alpha \in [0,1]$.
  
Analogously to the case of classical logic and classical set theory, also here we derive the \emph{intersection} and the \emph{union} of fuzzy sets from the above introduced logical operations. Let $A, B$ be fuzzy sets on $U$. Then the membership degree of an element $u\in U$ to their intersection
$A \cap B$ and their union $A\cup B$ is defined as follows:
\begin{align*}
    (A\cap B)(u) &= A(u) \otimes B(u) \ , \\
    (A\cup B)(u) &= A(u) \oplus B(u) \ .
\end{align*}


For any left-continuous t-norm $\otimes$, there is a unique binary operation $\Rightarrow$
called the \emph{residuum} (\emph{residuated implication}) of the t-norm $\otimes$ such that the following adjunction property holds
%
$$\gamma \otimes \alpha \le \beta\quad\textrm{if and only if}\quad \gamma \le \alpha \Rightarrow \beta \ . $$
%
Residuated implications possess special position among other fuzzy implications (\cite{Baczynski_Jayaram_2008c}) in capturing the multiple-valued modus ponens property. 

Stemming from a left-continuous t-norm $\otimes$, we may construct so-called \emph{residuated lattice} $\langle [0,1], \wedge, \vee, \otimes , \Rightarrow , 0 , 1 \rangle$ that is the underlying algebraic structure of truth-values of the respective fuzzy logic. Note, that the structure has two conjunctions, the \emph{strong} conjunction $\otimes$ and the \emph{weak} conjunction $\wedge$ that is present in any residuated lattice. Furthermore, we may define additional logical connectives, for instance, the \emph{residual negation} $\lnot$
and \emph{biresiduum}  $\Leftrightarrow$ that models the multiple-valued equivalence:
%
\begin{align*}
\lnot\alpha &= \alpha \Rightarrow 0 \ , \\
\alpha \Leftrightarrow \beta &= (\alpha \Rightarrow \beta) \wedge (\beta \Rightarrow \alpha)\ .
\end{align*}

For the particular \L ukasiewicz t-norm, the residual negation $\lnot$ leads to the  \emph{involutive} negation $\neg \alpha = 1-\alpha$ that obeys the law of double negation $\lnot\lnot\alpha = \alpha$. Let us denote such an involutive negation by $\sim$. The involutive negation can be used in deriving the dual t-conorm from a given t-norm (and vice-versa) or, in other words, in deriving the \emph{strong disjunction} from a given strong conjunction:
$$\alpha\oplus\beta = {\sim}({\sim}\alpha \otimes {\sim}\beta)\ .$$

Therefore, $\sim$ is an important unary connective not only for the \L ukasiewicz algebra but for all residuated lattices and we may freely extend such structures by the involutive negation for further use: $\langle [0,1], \wedge, \vee, \otimes , \Rightarrow, \sim 0 , 1 \rangle$. It does not mean that $\neg$ is not at disposal, it is always present via the definition recalled above and we have in general two negations that, in the case of the \L ukasiewicz algebra, coincide. The fact that the weak conjunction $\wedge$ and the weak disjunction $\vee$ are the lattice operations \emph{meet} (\emph{infimum})  $\land$ and the \emph{join} (\emph{supremum})  $\lor$ needs not further explanation.


\subsection{G\"odel Algebra}



Based on the selected t-norm, the \lfl{} package provides all the derived operations in a concise and extendable way. By calling the \code{algebra()} function with the name of the underlying t-norm, a named list of functions is obtained. The user may select from \code{"goedel"}, \code{"goguen"}, or \code{"lukasiewicz"} variant calling the respective G\"odel, Goguen (also often called \emph{product}), or the already above-mentioned \L ukasiewicz residuated lattices of operations. 

For example, the algebra based on the G\"odel t-norm, that is the standard minimum $\otimes = \wedge$, is obtained as follows:
%
\input{chunks/chunk-goedel.algebra}

As can be seen, the \code{algebra()} function returns a named list of the following functions:
\begin{itemize}
    \item \code{n}: (strict) negation defined as:
    $$
    \lnot\alpha = \begin{cases}
    1, & \text{if}\ \alpha = 0 \ , \\
    0, & \text{otherwise} \ ;
    \end{cases}
    $$
    \item \code{ni}: involutive negation defined as:
    ${\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \min\{\alpha, \beta\};$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \max\{\alpha, \beta\};$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta \ , \\
    \beta, & \text{otherwise}\ ;
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}


Functions \code{n} and \code{ni} accept a vector of numeric values as a single input and return a vector of the negated values. Two-argument functions \code{r} and \code{b} compute the desired operation element-wisely. Similarly, \code{pt}, \code{pc}, \code{pi}, and \code{ps} work element-wisely: they accept a vector of multiple arguments and compute the outputs of the desired operation on first elements of the input vectors, then on second elements, etc. until the end of the vectors is reached, which yields a vector of the resulting values. The vectorized variants of these functions, i.e., \code{t}, \code{c}, \code{i}, and \code{s} first merge\footnote{By merging here, we mean application of the chosen operation on all components of the given vector to obtain a single value.} all the input vector arguments and then calculate a single resulting value from them. See the example below for more information.
%
\input{chunks/chunk-goedel.algebra.examples}
%

Note that as the strong and weak conjunction coincide in the G\"{o}del algebra as well as the strong and weak disjunction coincide, also the following holds for the  functions in the \lfl{} R-package: \code{t} = \code{i}, \code{pt} = \code{pi}, \code{c} = \code{s}, and \code{pc} = \code{ps}.




\subsection{Goguen Algebra}



Goguen algebra is also often called the product algebra to emphasize that its central point -- the strong conjunction -- is nothing else but the standard product (multiplication) operation. Therefore, $\otimes = \cdot$ is also often called the product t-norm.
%
\input{chunks/chunk-goguen.algebra}
%
It is defined as follows:
\begin{itemize}
    \item \code{n}: (strict) negation defined as:
    $$
    \lnot\alpha = \begin{cases}
    1, & \text{if}\ \alpha = 0\ , \\
    0, & \text{otherwise}
\ ;    \end{cases}
    $$
    \item \code{ni}: involutive negation defined as:
    ${\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \alpha\beta;$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \alpha + \beta - \alpha\beta;$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta\ , \\
    \frac{\beta}{\alpha}, & \text{otherwise}\ ;
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}


\subsection{\L{}ukasiewicz Algebra}

The last implemented algebra is the \L{}ukasiewicz algebra that stems from the seminal work on 3-valued logic by Polish logician Jan \L ukasiewicz, see \cite{lukasiewicz19673}. Note, that \L ukasiewicz algebra forms so-called MV algebra (\cite{Chang:MV_1958}) that is the best generalization of the classical Boolean algebra. The implementation is provided as follows:

\input{chunks/chunk-lukasiewicz.algebra}
%
The particular functions are defined as follows:

\begin{itemize}
    \item \code{n}, \code{ni}: both negations are equally defined as:
    $\lnot\alpha = {\sim}\alpha = 1 - \alpha;$
    \item \code{t}, \code{pt}: vectorized and element-wise t-norm defined as:
    $\alpha \otimes \beta = \max\{0, \alpha + \beta - 1\};$
    \item \code{c}, \code{pc}: vectorized and element-wise t-conorm defined as: $\alpha \oplus \beta = \min\{1, \alpha + \beta\};$
    \item \code{r}: residuum defined as:
    $$
    \alpha\Rightarrow\beta = \begin{cases}
    1, & \text{if}\ \alpha\le\beta, \\
    1 - \alpha + \beta, & \text{otherwise;}
    \end{cases}
    $$
    \item \code{b}: bi-residuum;
    \item \code{i}, \code{pi}: vectorized and element-wise infimum defined as:
    $\alpha \land \beta = \min\{\alpha, \beta\};$
    \item \code{s}, \code{ps}: vectorized and element-wise supremum defined as:
    $\alpha \lor \beta = \max\{\alpha, \beta\}.$
\end{itemize}



\subsection{Partial Fuzzy Set Theory -- Handling of Undefined and Missing Values}

Partial logics (3-valued) were proposed to deal with undefined values of the distinct type (no matter the origin or interpretation of the undefinedness), such as  ``irrelevant'', ``inconsistent'', or ``unknown'' truth values. Detailed elaboration of such approaches is beyond the scope of the article so, we refer readers to the relevant source
\cite{CiucciDubois_Inf.Sci2013} and note, that three-valued partial logics were recently extended to partial fuzzy logics and partial fuzzy set theory (\cite{BehounekNovak:IEEE15},\cite{BehounekDankova:IPMU16}). 

We follow the above-mentioned works that considered distinct partial fuzzy logics, namely Bohvar, Soboci\'{n}ski, Kleene, and Nelson logic. Furthermore, as none of the referred logics was specifically designed for handling the missing values, two recent algebras for partial fuzzy logics were designed, in particular, the Lower estimation algebra (\cite{CaoStep:KSE18}), and the Dragonfly algebra (\cite{Step_etal_Dragon_IJAR2019}). Note, that in all cases, firstly an underlying algebra, e.g., G\"{o}del, Goguen or \L{}ukasiewicz is chosen and then the support is extended by a dummy value $\star$ in order to obtain $[0,1]\cup \star$. In terms of the missing values in \pkg{lfl}, we deal with extended support $[0,1]\cup \code{NA}$.

The implementation of algebras of functions in \pkg{lfl} treats missing values in such a way that if \code{NA} appears as a value to some operation,
it is propagated to the result. That is, any operation with \code{NA} results in \code{NA}, by default.
This scheme of handling missing values is equivalent to the choice of the Bochvar logic \citep{book:malinov} mentioned above.

The \code{sobocinski()}, \code{kleene()}, \code{nelson()}, \code{lowerEst()} and \code{dragonfly()} functions modify the algebra to
handle \code{NA}s in a different way than by the default choice. For example, Sobocinski algebra simply ignores \code{NA} values whereas Kleene algebra treats \code{NA} as ``unknown value'' similarly to the Bochvar one however, extreme points 0 and 1 have a specific position among other truth-values from the interval $[0,1]$. Dragonfly approach as well as the Lower estimation algebra combine 
Sobocinski and Bochvar approaches with the preservation of the ordering $0 \le \code{NA} \le 1$ which ,on the other hand, reflects some aspects of the Kleene algebra. The distinct algebraic incorporation of the treatment of missing values is provided in Tables~\ref{tab:res-negation}-\ref{tab:equivalences}.

\begin{table}
    \centering
    \caption{Handling of missing values by variants of residual negation}
    \label{tab:res-negation}
    \begin{tabular}{c|cccccc}
        $\lnot$     & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ \\
        \code{NA} & \code{NA} & $0$         & \code{NA} & $1$         & \code{NA} & $0$
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of involutive negation}
    \label{tab:inv-negation}
    \begin{tabular}{c|cccccc}
        $\sim$     & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ & $f(\alpha)$ \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA}
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of conjunctive operations}
    \label{tab:conjunctions}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\otimes$, $\land$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $0$         & $0$         & $0$         & $0$         & $0$         \\
        $\alpha$    & \code{NA} & \code{NA} & $\alpha$    & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $1$         & \code{NA} & \code{NA} & $1$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $0$         & \code{NA} & $0$         & $0$         & $0$         & $0$         & $0$         \\
        \code{NA} & $\beta$     & \code{NA} & $\beta$     & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $1$         & \code{NA} & $1$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} 
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of disjunctive operations}
    \label{tab:disjunctions}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\oplus$, $\lor$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $0$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $\alpha$    & \code{NA} & \code{NA} & $\alpha$    & \code{NA} & \code{NA} & $\alpha$    & $\alpha$    \\
        $1$         & \code{NA} & \code{NA} & $1$         & $1$         & $1$         & $1$         & $1$         \\
        \code{NA} & $0$         & \code{NA} & $0$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $\beta$     & \code{NA} & $\beta$     & \code{NA} & \code{NA} & $\beta$     & $\beta$     \\
        \code{NA} & $1$         & \code{NA} & $1$         & $1$         & $1$         & $1$         & $1$         \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} 
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of residuum}
    \label{tab:implications}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\Rightarrow$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $1$           & $1$         & $1$         & $1$         & $1$         \\
        $\alpha$    & \code{NA} & \code{NA} & $\lnot\alpha$ & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $1$         & \code{NA} & \code{NA} & $0$           & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $0$         & \code{NA} & $0$           & \code{NA} & $1$         & \code{NA} & $0$         \\
        \code{NA} & $\beta$     & \code{NA} & $\beta$       & \code{NA} & \code{NA} & $\beta$     & $\beta$     \\
        \code{NA} & $1$         & \code{NA} & $1$           & $1$         & $1$         & $1$         & $1$         \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA}   & \code{NA} & $1$         & $1$         & \code{NA} 
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Handling of missing values by variants of bi-residuum}
    \label{tab:equivalences}
    \begin{tabular}{cc|cccccc}
        \multicolumn{2}{c|}{$\Leftrightarrow$} & standard & \texttt{sobocinski} & \texttt{kleene} & \texttt{nelson} & \texttt{dragonfly} & \texttt{lowerEst} \\
        \hline
        $\alpha$    & $\beta$     & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ & $f(\alpha, \beta)$ \\
        $0$         & \code{NA} & \code{NA} & $0$         & \code{NA} & $1$         & \code{NA} & 0 \\
        $\alpha$    & \code{NA} & \code{NA} & $\neg \alpha \wedge \alpha$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        $1$         & \code{NA} & \code{NA} & $0$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $0$         & \code{NA} & $0$         & \code{NA} & $1$         & \code{NA} & 0 \\
        \code{NA} & $\beta$     & \code{NA} & $\neg \beta \wedge \beta$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & $1$         & \code{NA} & $0$         & \code{NA} & \code{NA} & \code{NA} & \code{NA} \\
        \code{NA} & \code{NA} & \code{NA} & \code{NA} & \code{NA} & $1$ & 1 & \code{NA} 
    \end{tabular}
\end{table}




By default, the functions in the structure that is obtained by calling the \code{algebra()} function simply propagate \code{NA} to the output. If some other handling of missing values is required, it can be done as follows. Firstly, the underlying algebra (G\"odel, Goguen or \L{}ukasiewicz) is created and then modified by applying one of the \code{sobocinski()}, \code{kleene()}, \code{nelson()}, \code{dragonfly()}, \code{lowerEst()} functions on it -- see the example:
%
\input{chunks/chunk-na.algebra}

\todo{musime tady poresit jeste usporadani - je potreba pozdeji u definice kvantifikatoru}


\section{Compositions of Fuzzy Relations}

\subsection{Fundamental compositions}

Compositions of fuzzy relation establish one of the fundamental blocks for mathematical fuzzy modeling, see \citep{Belohlavek_book2002,DeBaetsKerre_compoositions93}. Let us consider three non-empty universes $X,Y,Z$ and let $R\in\mathcal{F}(X\times Y)$ and $S\in\mathcal{F}(Y\times Z)$. In general, a composition of $R$ and $S$ results a fuzzy relation $R@S$ on $X\times Z$ so, it defines some appropriate relationship between elements from universes that were not connected before defining the composition. The obligatory example comes from the medical diagnosis where $X$ denotes the set of patients, $Y$ denotes the set of symptoms and $Z$ denotes the set of diseases \citep{BandlerKohout78}. 

%
\input{chunks/chunk-comp.data}
%

Below, we repeat the main four compositions that are implemented in \pkg{lfl} and that we stem from:
\begin{align}
   ( R\circ S )(x,z) & = \bigvee_{y\in Y} \left( R(x,y) \otimes S(y,z)\right) \ ,\label{form:circ}\\
    (R\lhd S )(x,z) & = \bigwedge_{y\in Y} ( R(x,y) \Rightarrow S(y,z)) \ , \label{form:lhd}\\
    (R\rhd S )(x,z) & = \bigwedge_{y\in Y} ( R(x,y) \Leftarrow S(y,z)) \ , \label{form:rhd}\\
    (R\squ S )(x,z) & = \bigwedge_{y\in Y} ( R(x,y) \Leftrightarrow S(y,z)) \label{form:squ}
\end{align}
where $\circ$ denotes the \emph{circlet} or \emph{basic composition} (also \emph{direct product}), $\lhd$ denotes the \emph{Bandler-Kohout subproduct} (also \emph{subdirect product}),  $\rhd$ denotes the \emph{Bandler-Kohout superproduct} (also (also \emph{supdirect product}), and finall, $\squ$ denotes the \emph{Bandler-Kohout square product}. 

Note that these four compositions were studied already in late 1970's and early 1980's \citep{BandlerKohout78, BandlerKohout80}, the first two of them ($\circ$ and $\lhd$) play an essential role in fuzzy inference mechanisms in the case of fuzzy inputs \citep{Pedrycz,Di-Nola_Pedrycz_Sessa_1989,StepBaluIEEE}, and their impact is essential fro distinct branches including the medical diagnosis, see \cite{Mak:JMS2015}. 

The main compositions, as defined above,  may be computed in \pkg{lfl} with the \code{compose()} function as follows:
%
\input{chunks/chunk-comp.basic}
%
The \code{type} argument must be equal to any of: \code{'basic'}, \code{'sub'}, \code{'super'} or \code{'square'}. The \code{compose()} function is merely a wrapper around the \code{mult()} function, which computes a customizable inner-product of two matrices. The \code{mult()} function takes two matrices as the arguments as well as a two-argument function, which is called for each combination of row and column. For instance, the basic composition may be alternatively computed as follows:
%
\input{chunks/chunk-comp.mult}
%
See below for some examples of more complicated compositions computed with the \code{mult()} function.

The fundamental fuzzy relational compositions (\ref{form:circ})-(\ref{form:squ}) can be directly used in the expert classification problem, and the above-mentioned medical diagnosis \citep{Mak:JMS2015} is only its special case where the symptoms are take as the features and the set of diseases is a special case of the set of classes. However, observing the particular formulas it is obvious, that the the huge gap between the existential quantifiers, represented by $\bigvee$ in (\ref{form:circ}), and the universal quantifier, represented by $\bigwedge$ in (\ref{form:lhd})-(\ref{form:squ}), may cause undesirable effect. In particular, the basic composition $\circ$ may detect too many suspicions as finding a single ``connecting'' feature (symptom) will be a very frequent case for many classes (diseases) while carrying all the expected features (symptoms) may be rather idealistic in practice too eliminating requirement. Thus, $\circ$ could nominate too many candidate classes while $\lhd, \rhd$, and $\squ$ may vice-versa, eliminate all possibilities. 

In order to get out of the problem, distinct extensions were defined recently. One direction of the extensions led naturally to the employment of generalized quantifiers that fill in the gap between the existential one and the universal one and offer a tool to find an appropriate balance. The other one is based on employing additional fuzzy relations containing another knowledge that may be helpful in reducing the suspicions detected by the basic composition.



\subsection{Compositions of more fuzzy relations}

Assume that we are given fuzzy relations $E,U\in\mathcal{F}(Y\times Z)$. The intended semantical meaning is such that $E(y,z)$ expresses the degree up to which $y$ is a feature that excludes the class $z$ from the possible candidates (so-called \emph{excluding feature}) and $U(y,z)$ expresses the degree up to which $y$ is a feature that is unavoidable for any object to be classified into the class $z$ (so-called \emph{unavoidable feature}). The approach using the first fuzzy relation has been described in \cite{CaoStep:ExcludfeatureESWA} while the work incorporating the latter one is very recent, see \cite{Step_etal:Unavoidable_KNOSYS2020}.

The compositions employing the concepts of excluding features and unavoidable features are defined as follows:
\begin{align}
    \RSE(x,z) & = (R\circ S) (x,z) \otimes \neg (R\circ E)\ ,\label{form:circ_excl}\\
    (R\circ S)^{\rhd U}(x,z) & = (R\circ S) (x,z) \otimes  (R\rhd U)\ ,  \label{form:circ_unavoid}\\
    \RSE^{\rhd U}(x,z) & = (R\circ S) (x,z) \otimes \neg (R\circ E) \otimes (R\rhd U)  \label{form:circ_excl_unavoid}
\end{align}
where the last one $\RSE^{\rhd U}$ combines the extra knowledge contained in both additional fuzzy relations. As we may see, these extension are mathematically rather straightforward combinations of the fundamental blocks that only allow other fuzzy relations to enter the constructions.

{\color{red}M: sem asi popis jake funkce toto implementuji v lflfl package}

\subsection{Compositions based on generalized quantifiers}

Another approach to avoid the undesirable effect of too many suspicions (classification candidates) provided by the basic composition and too few (or often none)  suspicions provided by the Bandler-Kohout products is based on employing generalized quantifiers such as {\tt A Few}, {\tt At least 2}, {\tt At least 40\%}, or {\tt Majority}.

Mathematically, the construction of the compositions based on a generalized quantifier $Q$ directly uses a \emph{symmetric fuzzy measure} $\mu$ on $Y$ that is a monotone (not necessarily additive) mapping on $Y$ with $\mu(\emptyset) = 0$ and $\mu(Y) = 1$ that is invariant with respect to cardinality. Then, the compositions $ R @^{Q} S $ where $ @\in \{\circ,\lhd \} $ is defined as follows:
    \begin{align}
    (R@^{Q} S) (x,z)  = \bigvee_{D\in
        \mathcal{P}(Y)\setminus\{\emptyset\}} \left( \left(\bigwedge_{y\in
        D} R(x,y) \circledast S(y,z) \right) * \mu(D) \right) \ ,\label{form_compQstandard}
    \end{align}
    where $ \circledast\in\{*,\rightarrow \} $ corresponds to the composition and $x\in X$ and $z\in Z$.

Due to the choice of the symmetric fuzzy measure, it is sufficient to consider relative cardinality and its modification by distinct non-decreasing functions $f:[0,1]\rightarrow [0,1]$ with $f(0) = 0$ and $f(1) = 1$. This is very important as it allows to calculate the composition as a Sugeno-integral \citep{KlementMesiarPap_IEEE2010}: 
\begin{align}
(R@^{Q} S) (x,z) =  \bigvee_{i=1}^n \left( \left( R(x,y_{\pi(i)})
\circledast S(y_{\pi(i)}, z) \right) * f(i/n) \right)
\label{form_calcu_@}
\end{align}
where $n$ is the cardinality of $Y$  $\pi$ is a permutation on $\{ 1, \dots , n\}$ such that
$R(x,y_{\pi(i)}) \circledast S(y_{\pi(i)},z) \ge R(x,y_{\pi(i+1)})
\circledast S(y_{\pi(i+1)},z)$ for any $i = 1, \dots , n-1$ where
$n$ denotes the cardinality of $Y$. The calculation according to (\ref{form_calcu_@}) is computationally much cheaper than by using (\ref{form_compQstandard}) and therefore, the implementation in \pkg{lfl} package is based on it. 


Note that an appropriate choice of $f$ naturally allows to modify $\mu$ in such a way that the constructed quantifier is equivalent to the existential or the universal one. For more details, we refer to \cite{Step2014,Cao2017b}.

{\color{red}M: sem asi popis jake funkce toto implementuji v lflfl package}



\subsection{Combined cases}

As the whole implementation in \pkg{lfl} is based on the functions, it is very easy to follow the block structure of distinct extension and thus, to call compositions using combinations of generalized quantifiers and additional fuzzy relations $E$ and $U$. For example, the use of the following composition:
   \begin{align*}
   (R\circ^{Q} S^{\backprime}E)^{\rhd U}(x,z) & = (R\circ^Q S) (x,z) \otimes \neg (R\circ E) \otimes (R\rhd U)  \label{form:circ_Q_excl_unavoid}
\end{align*}
turned to be very efficient in classifying dragonfly species as well as amphibian species, for appropriately chosen quantifiers, see \cite{Step_etal:Unavoidable_KNOSYS2020}.

{\color{red}M: sem asi popis jake funkce toto implementuji v lflfl package}







\section{Evaluative Linguistic Expressions}

Evaluative linguistic expressions \citep{Novak08} are expressions vaguely describing a position on a quantitative axis no matter that not necessarily the position can be numerically expressed, for example, we may consider expressions such as \emph{very nice}, \emph{not very intelligent}, \emph{extremely friendly}. Such expressions have either the form 
$$\langle \textrm{linguistic hedge}\rangle \langle \textrm{atomic expression}\rangle$$
or they are expressed as vague quantities also called fuzzy numbers \citep{Mares}. 

The latter case of fuzzy numbers in in the \pkg{lfl} package modelled with help of triangular or raised cosine that reach normality fuzzy set. The earlier case is based on a small set of atomic expression. Originally, the trichotomy {\tt Small, Medium, Big} was used however, later on, an extension to a pentachomy by adding  {\tt Lower Medium, Upper Medium} was proposed as an alternative for better distinction of medium-close values. The set of hedges, that serve as linguistic modifiers making the meaning of an atomic expression wider or narrower. If we consider an empty hedge as a special case between the hedges with narrowing effect and the hedges with widening effect, we obtain a linear order of hedges:
\begin{equation*}
\mathrm{Ex}\lh  \mathrm{Si} \lh \mathrm{Ve}\lh \mlingterm{empty}\lh
\mathrm{ML}\lh \mathrm{Ro}\lh  \mathrm{QR} \lh  \mathrm{VR}
\end{equation*}
where the abbreviations stand for {\tt extremely}, {\tt significantly}, {\tt very}, {\tt more or less}, {\tt roughly}, {\tt quite roughly}, and {\tt very roughly}, respectively. Note that not all hedges have to be necessarily used and some redundancy analysis results are valid only under assumptions that, e.g., require to omit the hedge VR.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=120mm]{DEE.png}
	\caption{Visual sketch of fuzzy sets representing some particular evaluative linguistic expressions. Specific defuzzification DEE is charted too.}\label{fig_dee}
\end{figure}

The ordering of hedges has an important role in the inference mechanism tailored to the linguistic fuzzy rules with the above mentioned evaluative linguistic expressions. It is based on specificity that is directly determined by the used hedge, assuming the same atomic expression is used. The more narrower the hedge, the more specific expression, that is modeled by a fuzzy sets that is in included in the fuzzy set modeling the less specific expression, see Fig.~\ref{fig_dee}.



\subsection{Linguistic Context} \label{sec:context}

The \emph{linguistic context} is a sort of extended notion to the notion of the universe in a sense that the universe is also accompanied with some fundamental points. For the case of modeling expressions on numerical axis, we may dare to restrict our focus to universes that are closed real intervals so, the universe would be $U = [v_L, v_R]$. In order to talk about linguistic expression, we need to add at least the third ``middle'' point that does not represent the center of the interval however, it denotes the most typical value for middle size objects in the given domain. Usually, as humans are more sensitive to smaller values than to the big ones, the middle point is closer to $v_L$ than to $v_R$. For example, we may consider the universe of pine trees $U=[3,80]$ (in meters) while the middle size pine tree would be rather around 15-20 meters than in the middle of the interval $U$. So, extending the context to $[v_L, v_C, v_R]$ is not just a redundant adding of the central point $v_C$ but a desirable specification of the prototypical middle point $v_c$. 

As the context has to reflect \emph{unilateral/bilateral} nature (if it respects the positive and negative values) or the decision whether we deal with \emph{trichotomy} or a \emph{pentachotomy} (pentachotomy would require two more such emphasized points), the order triplet $[v_L, v_C, v_R]$ is not the only choice but the most fundamental and the simplest form of a linguistic context. In particular,  four different contexts are supported in \pkg{lfl}, and the above-mentioned simplest context is the \emph{unilateral trichotomy} that is chosen by calling the function \code{ctx3()}. The higher density of atomic expressions can be obtained by adding expressions {\tt lower middle} and {\tt upper middle} which requires to consider \emph{unilateral pentachotomy} by calling the function \code{ctx5()}. The bilateral versions of the trichotomy and the pentachotomy allow to deal with expressions such as, negative small, positive medium, or negative very small and can be called by functions  \code{ctx3bilat()} and \code{ctx5bilat()}, respectively. The summary is provided below:
%
\begin{itemize}
    \item \code{ctx3(low, center, high)}: the unilateral trichotomy that enables the atomic expressions: \emph{small, medium, big}; 
    \item \code{ctx5(low, lowerCenter, center, upperCenter, high)}: the unilateral pentacho\-to\-my that enables the atomic expressions: \emph{small, lower medium, medium, upper medium, big}; 
    \item \code{ctx3bilat(negMax, negCenter, origin, center, max)}: the bilateral trichotomy that enables the atomic expressions: \emph{negative big, negative medium, negative small, zero, small, medium, big};
    \item \code{ctx5bilat(negMax, negUpperCenter, negCenter, negLowerCenter, origin, low\-erCenter, center, upperCenter, max)}: the bilateral pentachotomy that enables the atomic expressions: \emph{negative big, negative upper medium, negative medium, negative lower medium, negative small, zero, small, lower medium, medium, upper medium, big}.
\end{itemize}
%
The arguments of context creator functions have sensible defaults and need not be therefore explicitly stated in all cases:
%
\input{chunks/chunk-ctx}
%
Alternatively, the context may be automatically determined from data by calling the \code{minmax()} function, which creates the selected type of the context based on the minimum and maximum value found in data:
%
\input{chunks/chunk-minmax}
%
The \code{minmax()} function may be forced not to guess some values by specifying them explicitly as additional arguments:
%
\input{chunks/chunk-minmax2}

{\color{red}M: mam dotaz - nejde nastavit kontext rucne? ja nahore uvadim, priklad s velikosti borovice, proc kontext nelze brat jen jako redundantni vyznaceni prostredku, ale pak uvadis, ze se nic nenastavuje, ze to je automaticky uprostred, anebo z dat. To chapu tak, ze kdyz nemam data, musim volit to ekvidistantni rozdeleni bodu L-C-R popr. analogicky pro ostatni typy kontextu. }

\subsection{Evaluative Linguistic Expressions}

The atomic expressions, e.g., \emph{small}, \emph{medium} or \emph{big} in the case of the unilateral trichotomy, are according to the theory of evaluative linguistic expressions \citep{Novak08} modelled with help of the \code{horizon()} function. Horizon of the atomic expression is a function that represents basic limits of what humans treat as small, medium or big, see Fig.~\ref{fig:horizon}.
%
\input{chunks/chunk-horizon}
%

\begin{figure}
    \centering
    \includegraphics{figure/horizon-1.pdf}
    \caption{Horizons for the atomic expressions \emph{small}, \emph{medium} and \emph{big} in the unilateral trichotomy (\code{ctx3(0, 0.5, 1)})}
    \label{fig:horizon}
\end{figure}

A particular linguistic expression is obtained after the application of the linguistic hedge. So, in \pkg{lfl}, the \code{hedge()} function works as a modifier function, that is applied to a particular horizon. For instance, the function that represents the ``very small'' expression can be obtained as follows:
%
\input{chunks/chunk-hedge}
%
Such an approach gives the user a detailed control of the creation of a linguistic expression, which may be useful for experimenting with novel expressions. However, it may be tedious to manually create the functions for a routine use. Therefore, the \code{lingexpr()} function provides a shortcut for creation of pre-defined expressions:
%
\input{chunks/chunk-lingexpr}

An expression consisting of an atomic expression only, is constructed using an empty hedge:
%
\input{chunks/chunk-emptyhedge1}
%
or equivalently:
%
\input{chunks/chunk-emptyhedge2}
%
Figure~\ref{fig:lingexpr} shows all linguistic expressions of the unilateral trichotomy context (\code{ctx3}).

\begin{figure}
    \centering
    \includegraphics{figure/lingexpr1-1.pdf}
    \includegraphics{figure/lingexpr2-1.pdf}
    \includegraphics{figure/lingexpr3-1.pdf}
    \caption{All pre-defined linguistic expressions for the unilateral trichotomy context \code{ctx3}}
    \label{fig:lingexpr}
\end{figure}


\subsection{Other Functions}

For the sake of completeness, the \pkg{lfl} package provides tools for the creation of triangular or {\color{red}raised-cosinal} functions. Both \code{triangular()} and \code{raisedcosinal()} functions take three arguments, \code{lo}, \code{center}, \code{hi}, which fully parameterize the shape of the resulting function. See the example below as well as Figure~\ref{fig:triangular} for more detail. Note also that the \code{lo} and \code{hi} parameters may be set to an infinite value (\code{-Inf} resp. \code{Inf}), which causes the particular tail to be constantly equal to 1.
%
\input{chunks/chunk-triangular}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics{figure/triangular1-1.pdf}
        \caption{\code{triangular(0, 0.5, 1)}}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics{figure/triangular2-1.pdf}
        \caption{\code{triangular(-Inf, 0.5, 1)}}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics{figure/raisedcos1-1.pdf}
        \caption{\code{raisedcosinal(0, 0.5, 1)}}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics{figure/raisedcos2-1.pdf}
        \caption{\code{raisedcosinal(0, 0.5, Inf)}}
    \end{subfigure}
    \caption{Triangular and raised-cosinal functions}
    \label{fig:triangular}
\end{figure}

\subsection{Specific expressions}

{\color{red}M: tady musíme popsat výrazy typu {\tt any} nebo {\tt undef}, protoze se dale na ne budeme odkazovat}

\subsection{Batch Transformations of Data to Membership Degrees of Fuzzy Sets}

Practical applications often require to transform data into multiple fuzzy sets. The \pkg{lfl} package provides the \code{lcut()} and \code{fcut()} functions to perform such transformations. {\color{blue}Both functions transform vectors (numeric, logical or factors), matrices or data frames into an \code{fsets} object. Such an object is a data frame with each column representing a single fuzzy set. The values are from the $[0, 1]$ interval and they are equal to the membership degrees of the point of the universe (context) to the particular fuzzy sets. These functions are the fuzzy-counterparts of the well known \code{cut()} operation of the base \R{}.} {\color{red}M: modrou cast jsem prespal citelne, koukni, zda to obsahove vystihuje presne to, co to v R-ku vazne dela}

For logical input, the \code{lcut()} function returns two columns of 0s and 1s: these columns represent (crisp) truth degrees equivalent to the original input and its negation, respectively. The name of the columns is either specified by the user in the \code{name} argument, or derived from the given data argument automatically:
%
\input{chunks/chunk-lcut.logical}
%
The factor input is dichotomized in the result:
%
\input{chunks/chunk-lcut.factor}

For numeric input, the \code{lcut()} function performs transformation to linguistic expressions similarly as described in the previous section. For this step, a linguistic context must be provided (see Section~\ref{sec:context}) and the atomic expressions and hedges may be specified. If the context is not provided, the context is determined automatically using the \code{minmax()} function described in Section~\ref{sec:context} and all relevant atomic expressions and hedges are applied:
%
\input{chunks/chunk-lcut.numeric}

If data frame is to be processed with the \code{lcut()} function, the result is created per column. Also note that the names of the resulting variables are derived from the column names of the input data frame:
%
\input{chunks/chunk-lcut.data.frame}
%
The given contexts, atomic expressions and hedges are recycled for each input numeric column. If a different setting is needed for each column, the arguments should be given as named lists as follows:
%
\input{chunks/chunk-lcut.data.frame2}

{\color{red}M: TADY POKRACUJU PRISTE}

The \code{fsets} object returned by the \code{lcut()} function (and the \code{fcut()} function as well, see below) handles an additional information, the \code{vars} and \code{specs} attributes. In particular, \code{vars} is a character vector that assigns the original data name to each of the resulting column of membership degrees. In other words, the \code{vars} vector specifies the equivalence classes of fuzzy sets that were originated from the same data:
%
\input{chunks/chunk-lcut.vars}
%
The \code{specs} attribute returns a matrix that encodes a relation of \emph{specificity} {\color{red}(see Section~\ref{sec:lingexpr-theory})} between the columns of the \code{fsets} object (some columns and rows are omitted for brevity): {\color{red}M: navic relation specifity jsem jako pojem asi formalne nezavedli}
%
\input{chunks/chunk-lcut.specs}
%
As can be seen, the 4th fuzzy set (\code{ve.sm.age}) is more specific than 5th (\code{sm.age}), which is more specific than the 6th fuzzy set (\code{ro.sm.age}).


The \code{fcut()} function works identically as \code{lcut()} for logical and factor input:





\section{Fuzzy Association Rules}



The association rules \citep{Agrawal:assoc_rules} need not be introduced in detailed, we only recall the fact that firstly the method appeared under the name GUHA \citep{Hajek:GUHA,HajekHavranek_GUHA} and furthermore, we recall basic principles of how this method finds  distinct statistically
approved associations between attributes of given objects. With help of \pkg{lfl} the method can be used also in the fuzzy setting, i.e., for graded properties. 


The crisp version of association rules deals with
Table~\ref{Tab:guha_exampleI} where $o_1,\dots, o_n$ denote objects,
$X_1,\dots , X_m$ denote independent boolean attributes, $Y_1,
\dots, Y_q$ denote the dependent boolean attributes, and
finally. Symbols $a_{ij}$ and $b_{ij}$ are values from $ \{ 0,1 \}$ that denote
whether the $i$-th object $o_i$ carries attribute $X_j$ or $Y_j$, respectively. Each object can be represented as a boolean vector with $m+q$ components.

\begin{table}[ht]
  \centering
\begin{tabular}{|l|ccc|ccr|}
    \hline
          & $X_1$ & $\ldots$ & $X_m$ & $Y_1$ &  $\ldots$ & $Y_q$\\
    \hline
    $o_1$ & $a_{11}$ & $\ldots$ & $a_{1m}$ & $b_{11}$ &  $\ldots$ & $b_{1q}$\\
    $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$\\
    $o_n$ & $a_{n1}$ & $\ldots$ & $a_{nm}$ & $b_{n1}$ &  $\ldots$ & $b_{nq}$\\
    \hline
\end{tabular}\caption{The table objects and attributes.} \label{Tab:guha_exampleI}
\end{table}

As the GUHA method deals with boolean attributes and features are often taking values from a real universe, the attributes are usually partitioned to intervals. Then the method seeks for associations:\begin{equation} {\tt A}(X_{i1},\ldots,X_{ip})\simeq
{\tt B}(Y_k) \label{form:assoc_general}
\end{equation}
where ${\tt A}$ is a predicate with conjunctively connected variables $X_{i1},\ldots,
X_{ip}$ (for $p\leq m$), and ${\tt
B}$ is a predicate with variables $Y_k$, for $k$ taking values from 1 to $q$. In order to mine such associations, a four-fold 
Table~\ref{Tab:4fold-guha} is constructed.
\begin{table}[ht]
\centering
\begin{tabular}{|r|lc|}
\hline
  & ${\tt B}$ & $\text{not}\; {\tt B}$ \\
    \hline
    ${\tt A}$ & $a$ & $b$\\
    $\text{not}\; {\tt A}$ & $c$ & $d$\\
    \hline
\end{tabular}
\caption{Four-fold table for mining linguistic associations.} \label{Tab:4fold-guha}
\end{table}



The number of synchronous occurrences of ${\tt A}$ and ${\tt B}$ is denoted by $a$ in Table~\ref{Tab:4fold-guha}, $b$ denotes the number of occurrences of ${\tt A}$ while ${\tt
B}$ does not hold, numbers $c$ and $d$ are determined analogously. Often, only numbers $a$ and $b$ are important for distinct qualitative measures. For instance, the relationship between ${\tt A}$ and ${\tt B}$ may be obtained with help of the \emph{binary multitudinal quantifier} $\simeq\
:=\ \sqsubset^{\gamma}_{r}$ that confirms the association if:
$$\frac{a}{a+b}>\gamma \qquad \textrm{and} \qquad \frac{a}{n}>r,$$
where $\gamma \in [0,1]$ is a \emph{degree of confidence} and
$r\in[0,1]$ is a \emph{degree of support}.



In order to soften the binary character of the associations and the sensitivity to the  partitioning of the universes into intervals, distinct approaches to fuzzy associations were employed
\cite{Sudkamp:2005,Kupka:NNW2010,Glass:FSS2008}.
The \pkg{lfl} package adopts the approach published in \cite{StepBurda:FRBE_FSS} because  it directly uses the theory of evaluative
linguistic expressions. The attributes are not boolean anymore. Recalling the example from \cite{StepBurda:FRBE_FSS}, we may consider, independent variables BMI (Body Mass Index) and Chol (cholesterol level), and the dependent variable BP (blood pressure) and the attributes such as $\mathrm{BMI}^{\mathrm{Ve Bi}}$,
$\mathrm{BP}^{\mathrm{Me}}$, $\mathrm{Chol}^{\mathrm{Si Bi}}$
etc. that are defined by the fuzzy sets modeling  the particular evaluative expressions. In such a way, the authors obtained Table~\ref{Tab:FuzzyGuhaComplete}  with membership degrees $a_{ij} \in [0,1]$.

\begin{table}[ht]
  \centering
\begin{tabular}{|l| c c c |c c c|}
    \hline
          & $\mathrm{BMI}^{\mathrm{Ex
Sm}}$ &  \ldots & $\mathrm{Chol}^{\mathrm{Ex Bi}}$ & $\mathrm{BP}^{\mathrm{Ex Sm}}$ & \ldots & $\mathrm{BP}^{\mathrm{Ex Bi}}$\\
    \hline
    $o_1$ & 0.5 & \ldots & 0 & 1 & \ldots & 0\\
    $o_2$ & 0.8 & \ldots & 0  & 0.4 & \ldots & 0\\
    $o_3$ & 0 &  \ldots & 0.1  & 0 & \ldots & 0.4\\
    $o_4$ & 0 &  \ldots & 0.4  & 0 & \ldots & 0.3\\
    $o_5$ & 0.6  & \ldots & 0  & 1 & \ldots & 0\\
    $\vdots$  & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$& $\vdots$\\
    $o_n$ & 0 & \ldots & 0  & 0.5 & \ldots & 0 \\
    \hline
\end{tabular}
\caption{An example of the table with fuzzy attributes for fuzzy associations mining, ref. \cite{StepBurda:FRBE_FSS}. } \label{Tab:FuzzyGuhaComplete}
\end{table}



Table~\ref{Tab:4fold-guha} is
constructed for the fuzzy case in the same way using the cardinality, i.e., values $a,b,c,d$ are summations of membership degrees. The conjunctive aggregations of the truth-values of 
${\tt A}$ and ${\tt B}$ 
were modeled by the minimum operation however, other t-norms \citep{KlementMesiarPap00} may be considered as well. So, if the membership degree of $o_i$ to ${\tt A}$ is
0.4 and its membership degree to ${\tt B}$ is 0.7, the value that
enters the summation equals to $\min\{ 0.4,0.7\} = 0.4$. If we sum up such values over all objects, we obtain the number $a$ in
Table~\ref{Tab:4fold-guha}. The other numbers are determined in an analogous way. 


The advantage of associations (\ref{form:assoc_general}) is that each of them can be interpreted as the fuzzy rule:
$$\mathcal{R}\defeq\IFRuleMore{X_{i1}}{\mathcal{A}_{i1}}{X_{ip}}{\mathcal{A}_{ip}}{Y_k}{\mathcal{B}_{ik}}$$ that may be used in the inference systems for approximate reasoning. Variables $X\neq X_{i1},\ldots , X_{ip}$ that do not explicitly appear in the antecedent are assigned the expression {\tt any}. 



{\color{red}tady nasypej implementaci v R, pokud v ni jsou treba ruzne dalsi miry kvality (zamerne se vyhybam pojmu kvantifikator, at se to neplete s temi kvantifikatory u kompozic), tak by je bylo dobre nahore take uvest}





\section{Perception-based Logical Deduction}




We briefly recall, that the (specificity) ordering of linguistic expressions is based on the ordering of hedges under the assumption that we consider two expressions based on the same atomic expression, i.e.,$
\mathcal{A}_i \lle \mathcal{A}_k$
for  $\mathcal{A}_i:= \mlingterm{hedge}_i
\mathcal{A}$ and $\mathcal{A}_k:= \mlingterm{hedge}_k \mathcal{A}$ with $\mathcal{A}$ being an atomic expression and
$\mlingterm{hedge}_i\lh \mlingterm{hedge}_k$ where
\begin{equation*}
\mathrm{Ex}\lh  \mathrm{Si} \lh \mathrm{Ve}\lh \mlingterm{empty}\lh
\mathrm{ML}\lh \mathrm{Ro}\lh  \mathrm{QR} 
\lh \mathrm{VR} .
\end{equation*}
The expression {\tt any} has a unique positions as $\mathcal{A}_i \lle \mbox{{\tt any}}$ for arbitrary
expression $\mathcal{A}_i$.

The specificity ordering principle, i.e., evaluative expressions of the same type are ordered according to their hedges and expressions with different atomic expressions are incomparable), is preserved also for the multiple-variable case where the same atomic expression needs to be on each axis and the ordering $\lh$ has to be preserved also for all variables (hedges), otherwise again, the expressions are incomparable. 



Fuzzy rules with evaluative expressions are gathered to a fuzzy rules base that is here called the \emph{linguistic description}, $\LD=
\{\mathcal{R}_1, \ldots, \mathcal{R}_K\}$:
\begin{align}
\mathcal{R}_1&\defeq\IFRule{x}{\mathcal{A}_1}{y}{\mathcal{B}_1},\notag\\
&\makebox[55mm]{\dotfill}\label{form.lingdesc}\\
\mathcal{R}_K&\defeq\IFRule{x}{\mathcal{A}_K}{y}{\mathcal{B}_K}\notag 
\end{align}
with $x,y$ take values from universes $X$ and $Y$, respectively. For further purposes, we will need to define the \emph{topic of linguistic description }\LD\ as the set of antecedent fuzzy sets
$T^{\LD} =\{ A_j \mid j=1, \ldots, K\}$ where $A_j$ models the expression $\mathcal{A}_j$.






Given a linguistic description and an input $x_0\in X$, may order the elements of the topic w.r.t. the input: $ A_{i} \llp A_{k}$ for $A_i,A_k\in \T^{\LD}$ if 
\begin{equation*}
\mbox{either } \quad  A_{i}(x_0)> A_{k}(x_0); 
\quad \mbox{or }\quad A_{i}(x_0) = A_{k}(x_0) \; \mbox{and }\
\mathcal{A}_i\lle \mathcal{A}_k.
\end{equation*}
Again, the extension to more variables is straightforward and component-wise with the use of the minimum t-norm to aggregate the membership degrees on the individual axes:
\begin{equation*}\label{form:compExtens}
A_i(x_0) =\bigwedge_{j=1}^m A_{ij}(x_{0j}), \quad \mobx{\rm with}\quad X = X_1\times \cdots
\times X_m , \quad x_0 =  (x_{01}, \dots, x_{0m})\ .
\end{equation*}

{\color{red}TADY POKRACUJU PROSTE, NIZE MAM JEN NAKOPIROVANE VECI K UCESANI}


\emph{The perception-based logical deduction} (abbr. PbLD) is a specific
inference method aimed at the derivation of results based on
linguistic descriptions, see \cite{Novak:PbLD,novper:intellig04}. In
this Section, we present the PbLD in a variant that was in detail
studied in \cite{RedundancyFSS_2015}. However, note that there are
also other variants, see e.g. \cite{DvoStep:PbLD2015}.

A \emph{perception} is understood as an evaluative expression
assigned to a given input value (observation) in a given context
(universe). The perception is always chosen from evaluative
expressions which occur in the topic of a given linguistic
description \cite{nov_dvo:commonsense2011}. The perception is
determined by a special function called the \emph{local perception}
which is based on the ordering $\llp$.
\begin{defn}
{\rm Let $\LD$ be a linguistic description (\ref{form.lingdesc}).
The \emph{local perception} function is a mapping $\LPerc^{LD}: U
\to \mathcal{P}(T^{\LD})$ assigning to each value $u_0\in U$ a
subset of antecedent fuzzy sets that are minimal wrt.~the ordering
$\llp$, i.e.,}
\begin{multline}\label{eq.lperc}
\LPerc^{\LD}(u_0) =  \{ A_i \in T^{\LD}\mid A_i(u_0)>0 \ \&\
\forall A_j\in T^{\LD}: (A_j\llp A_i) \Rightarrow (\mathcal{A}_j =
\mathcal{A}_i) \}.
\end{multline}
\end{defn}

The local perception function has a key role in the definition of
the PbLD inference mechanism. It can be viewed as a function that
``fires'' chosen rules.

\begin{defn}
{\rm Let $\LD$ be a linguistic description (\ref{form.lingdesc}).
Let us be provided with an observation $u_0\in U$. Then, the
\emph{rule of perception-based logical deduction} is given as
follows:}
\begin{equation}\label{rPbLD}
r_{PbLD}: \frac{\LPerc^{\LD}(u_0), \LD}{C}
\end{equation}
\rm{where}
\begin{equation*}
C = \bigcap \{C_{i_\ell}\mid
C_{i_\ell}(v)=A_{i_\ell}(u_0)\rightarrow B_{i_\ell}(v) \; \  \& \; \
A_{i_\ell}\in \LPerc^{\LD}(u_0) \}, \quad v\in V,
\end{equation*}
{\rm where $\rightarrow$ is the \L ukasiewicz implication and $\cap$
is the G\"{o}del intersection.}
\end{defn}

Informally, $C$ is the conclusion which corresponds to the
observation in the following way. Inputs to this inference rule are
the linguistic description $\LD$ and the local perception
$\LPerc^{\LD}(u_0)$ given by (\ref{eq.lperc}). This local perception
is formed by a set of fuzzy sets $A_{i_{\ell}}$, $\ell = 1, \ldots,
L$, that are chosen from the topic of $\LD$ according to
(\ref{eq.lperc}). Formula (\ref{eq.lperc}) chooses these
antecedents, which best fit the given numerical input $u_0$ and
thus, they should be fired. Then the individual conclusions
$C_{i_{\ell}}$ contained in $C$ are computed as
$A_{i_\ell}(u_0)\rightarrow B_{i_\ell}(v)$ for all $v\in V$. It
means that for each $A_{i_{\ell}}\in\LPerc^{\LD}(u_0)$ we take the
$i_{\ell}$-th IF-THEN rule from $\LD$ and compute the conclusion,
for the time being forgetting other IF-THEN rules from $\LD$.


Finally, the inferred output is defuzzified. This is done by the
\emph{Defuzzification of Evaluative Expressions} (DEE) that has been
designed specifically for the outputs of the PbLD inference
mechanism\footnote{In the case of the FRBE method, the
defuzzification DEE is applied after the inference, in order to
obtain crisp weights $w_{AR}, w_{ES}, w_{Theta}$ and $w_{RW}$.}, see
e.g. \cite{Novak:PbLD,step:TS_FSS2011}. As the PbLD stems from
the implicative rules
\cite{BodenhoferDankovaStepnickaNovak07,Jones2009} and not from the
Mamdani-Assilian ones (details can be found below), the
defuzzification cannot use principles similar to the
\emph{Center-Of-Gravity} defuzzification but has to focus on sets of
elements with maximal membership degrees. In principle, this
defuzzification is a combination of \emph{First-Of-Maxima} (FOM),
\emph{Mean-Of-Maxima} (MOM) and \emph{Last-Of-Maxima} (LOM) that are
applied based on the classification of the inferred output fuzzy
set. Particularly, if the inferred fuzzy set is of the type {\tt
Small}, the LOM is applied; if the inferred output is of the type
{\tt Medium}, the MOM is applied; and finally, if the inferred
output is of the type {\tt Big}, the FOM is applied, see
Figure~\ref{fig:Dee}. Generally, a simple application of MOM could be
also possible but it did not confirm to be better on experimental
evaluations \cite{step:TS_FSS2011}. It is important to
mention, that PbLD as well as DEE are freely available as an open-source software
in the \emph{lfl} package \cite{lfl2015,lfl:cran} for the R statistical environment
\cite{R}.


\section{Fuzzy Rule-based Ensemble for Time Series Prediction}



\section{Conclusion}



\section*{Acknowledgments}

Partial support of Czech Science Foundation through the grant 20-07851S is gratefully announced.


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{bibliography}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\section{More technical details} \label{app:technical}

Appendices can be included after the bibliography (with a page break). Each
section within the appendix should have a proper section title (rather than
just \emph{Appendix}).

For more technical style details, please check out JSS's style FAQ at
\url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
which includes the following topics:
\begin{itemize}
  \item Title vs.\ sentence case.
  \item Graphics formatting.
  \item Naming conventions.
  \item Turning JSS manuscripts into \proglang{R} package vignettes.
  \item Trouble shooting.
  \item Many other potentially helpful details\dots
\end{itemize}


\section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
\verb|\citealp| etc.\ (and never hard-coded). This commands yield different
formats of author-year citations and allow to include additional details (e.g.,
pages, chapters, \dots) in brackets. In case you are not familiar with these
commands see the JSS style FAQ for details.

Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
when acquiring the entries automatically from mixed online sources. However,
it is important that informations are complete and presented in a consistent
style to avoid confusions. JSS requires the following format.
\begin{itemize}
  \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
    be used in the references.
  \item Titles should be in title case.
  \item Journal titles should not be abbreviated and in title case.
  \item DOIs should be included where available.
  \item Software should be properly cited as well. For \proglang{R} packages
    \code{citation("pkgname")} typically provides a good starting point.
\end{itemize}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
